{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing REPBULIC hOCR Files\n",
    "\n",
    "The scans of the printed RSG volumes have the following characteristics\n",
    "\n",
    "- all scans:\n",
    "  - have two pages per scan\n",
    "  - have up to 4 columns per scan, 2 per page \n",
    "  - full scan is around 4800 pixels wide, left page is up to pixel 2400, right page is from pixel 2400 (roughly)\n",
    "- scans of index pages\n",
    "  - have no page numbers\n",
    "- scans of resolution pages\n",
    "  - have page numbers (left-side page is even, right-side page is odd)\n",
    "  \n",
    "### Columns\n",
    "\n",
    "The scans are normalized such that the columns are straight. The text width should be around 1000 pixels. Some columns are not cut out properly, resulting in columns that are either to small (some of the column text is missing), or too wide (the hOCR output contains partial texts from two columns)\n",
    "\n",
    "### Index pages\n",
    "\n",
    "- start of entry: \n",
    "  - start left alignment\n",
    "- end of entry:\n",
    "  - end of line possibly before end of text column. \n",
    "  - One or more page numbers\n",
    "\n",
    "\n",
    "### Resolution pages\n",
    "\n",
    "- header:\n",
    "  - next top of page (less than 350 pixels from the top)\n",
    "  - page has header with:\n",
    "    - even numbered pages: date page_number year\n",
    "    - odd numbered pages: year page_number date\n",
    "  - columns have half of page header, e.g.:\n",
    "    - even numbered pages: \n",
    "      - first column: date left aligned and part of page_number right aligned\n",
    "      - second column: part of page_number left aligned and year right aligned\n",
    "    - odd numbered pages: \n",
    "      - first column: year left aligned and part of page_number right aligned\n",
    "      - second column: part of page_number left aligned and date right aligned\n",
    "      \n",
    "### Viewer\n",
    "\n",
    "- page viewer: https://images.huygens.knaw.nl/assets/argos/index.html\n",
    "- list of page URLs: https://images.huygens.knaw.nl/api/argos\n",
    "\n",
    "\n",
    "### National Archive site\n",
    "\n",
    "- search in the archive: https://www.nationaalarchief.nl/onderzoeken/index/nt00444?searchTerm=\n",
    "- search the index: https://www.nationaalarchief.nl/onderzoeken/zoekhulpen/voc-opvarenden\n",
    "- example page: https://www.nationaalarchief.nl/onderzoeken/index/nt00444/d110980c-c864-11e6-9d8b-00505693001d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reload library is just used for developing the REPUBLIC hOCR parser \n",
    "# and can be removed once this module is stable.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# This is needed to add the repo dir to the path so jupyter\n",
    "# can load the republic modules directly from the notebooks\n",
    "import os\n",
    "import sys\n",
    "repo_dir = os.path.split(os.getcwd())[0]\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "#from republic.parser.generic_hocr_parser import make_hocr_doc\n",
    "import republic.parser.republic_page_parser as page_parser\n",
    "import republic.parser.republic_paragraph_parser as paragraph_parser\n",
    "import republic.parser.republic_file_parser as file_parser\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import republic.elastic.republic_elasticsearch as rep_es\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "\n",
    "# The hOCR file name contains relevant information for parsing. Here's an example:\n",
    "# NL-HaNA_1.01.02_3780_0016.jpg-0-251-98--0.40.hocr\n",
    "\n",
    "# NL-HaNA_1.01.02 is the name of the archive\n",
    "# 3780_0016 identifies the specific page with a specific contract\n",
    "# 0-251-98--0.40 identifies four aspects:\n",
    "#   1. the number of the column (0)\n",
    "#   2. the offset from the left (251)\n",
    "#   3. the offset from the top (98)\n",
    "#   4. and the slant (-0.40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading column scans for a single volume\n",
    "\n",
    "1. get scan file info\n",
    "    - scan number, page number, page side, column number, slant, page\n",
    "2. iterate over pages\n",
    "    - create hocr_page\n",
    "    - determine page type: index, resolution, other\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scan files: 2161\n"
     ]
    }
   ],
   "source": [
    "#from republic.parser.generic_hocr_parser import make_hocr_doc\n",
    "import republic.parser.republic_page_parser as page_parser\n",
    "import republic.parser.republic_file_parser as file_parser\n",
    "from republic.config.republic_config import base_config, set_config_year\n",
    "\n",
    "import copy\n",
    "\n",
    "year = 1725\n",
    "data_dir = \"/Users/marijnkoolen/Data/Projects/REPUBLIC/hocr\"\n",
    "\n",
    "\n",
    "def get_pages_info(config):\n",
    "    scan_files = file_parser.get_files(config[\"data_dir\"])\n",
    "    print(\"Number of scan files:\", len(scan_files))\n",
    "    return file_parser.gather_page_columns(scan_files)\n",
    "\n",
    "year_config = set_config_year(base_config, year, data_dir)\n",
    "pages_info = get_pages_info(year_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Page Data in Elasticsearch\n",
    "\n",
    "Index the resolution volumes at the page level.\n",
    "\n",
    "Every scan contains two pages. Since index terms reference page numbers, we want to be able to access individual pages for later matching.\n",
    "\n",
    "### Determining Page Type\n",
    "\n",
    "We want to parse index pages differently from resolution pages and filter out non-text pages and pages where the columns are not properly identified.\n",
    "\n",
    "So a first step is to use the page layout and content to distinguish pages containing indices from pages containing resolution summaries. There are also title pages, that indicate where a new part starts (e.g. indices, resolutions of the first half of the year, resolutions of the second half of the year).\n",
    "\n",
    "For examples of title pages, see: https://www.nationaalarchief.nl/onderzoeken/archief/1.01.02/inventaris?inventarisnr=3780&scans-inventarispagina=43&activeTab=gahetnascans#tab-heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_pages = {\n",
    "    154: {\n",
    "        \"scan_num\": 77,\n",
    "        \"page_num\": 154,\n",
    "        \"type_page_num\": 64,\n",
    "        \"special_type\": \"table\",\n",
    "    },\n",
    "    155: {\n",
    "        \"scan_num\": 77,\n",
    "        \"page_num\": 155,\n",
    "        \"type_page_num\": 65,\n",
    "        \"special_type\": \"table\",\n",
    "    },\n",
    "    156: {\n",
    "        \"scan_num\": 78,\n",
    "        \"page_num\": 156,\n",
    "        \"type_page_num\": 66,\n",
    "        \"special_type\": \"table\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year-1725-scan-6-even\n"
     ]
    }
   ],
   "source": [
    "# What page info do we get\n",
    "import json\n",
    "\n",
    "for page_id in pages_info:\n",
    "    if pages_info[page_id][\"scan_num\"] > 6 or pages_info[page_id][\"scan_num\"] < 6:\n",
    "        continue\n",
    "    print(page_id)\n",
    "    #print(json.dumps(pages_info[page_id], indent=2))\n",
    "    for column_info in pages_info[page_id][\"columns\"]:\n",
    "        #print(json.dumps(column_info, indent=2))\n",
    "        column_hocr = page_parser.get_column_hocr(column_info, year_config)\n",
    "        #print(json.dumps(column_hocr, indent=2))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import republic.elastic.republic_elasticsearch as rep_es\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "rep_es.parse_pre_split_column_inventory(es, pages_info, year_config, delete_index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Incorrect Page Type Assignments and Numbered Page Numbers\n",
    "\n",
    "**Problem 1**: For some pages the page type may be incorrectly identified (e.g. an index page identified as a resolution page or vice versa). This mainly happens on pages with little text content or pages where the columns are misidentified. \n",
    "\n",
    "**Solution**: Using the title pages as part separators, and knowing that the indices precede the resolution pages, we can identify misclassified page and correct their labels.\n",
    "\n",
    "**Problem 2**: Some pages are duplicates of the preceding scan. When the page turning mechanism fails, subsequent scans are images of the same two pages. Duplicate page shoulds therefore come in pairs, that is, even and odd side of scan $n$ are duplicates of even and odd side of scan $n-1$. Shingling or straightforward text tiling won't work because of OCR variation. Many words may be recognized slightly different and lines and words may not align.\n",
    "\n",
    "**Solution**: Compare each pair of even+odd pages against preceding pair of even+odd pages, using Levenshtein distance. This deals with slight character-level variations due to OCR. Most pairs will be very dissimilar. Use a heuristic threshold to determine whether pages are duplicates.\n",
    "\n",
    "**Problem 3**: A second problem is that page numbers of numbered pages are reset per part, starting from page 1, but the title page separating the first and second halves of the year should not reset the page numbering. \n",
    "\n",
    "**Solution**: Iterate over the pages, using a flag to keep track of whether we're in the indices part or a resolution part. If the title page is within the resolution part, update the page numbers by incrementing from the previous page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import republic.elastic.republic_page_checks as page_checks\n",
    "\n",
    "page_checks.correct_page_types(es, year_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_checks.detect_duplicate_scans(es, year_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_checks.correct_page_numbers(es, year_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Resolutions From Pages\n",
    "\n",
    "Identify:\n",
    "\n",
    "- resolution dates\n",
    "- resolution participant lists\n",
    "- resolution text blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from republic.fuzzy.fuzzy_context_searcher import FuzzyContextSearcher\n",
    "from republic.fuzzy.fuzzy_person_name_searcher import FuzzyPersonNameSearcher\n",
    "from republic.model.republic_phrase_model import resolution_phrases, participant_list_phrases, spelling_variants\n",
    "\n",
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.8,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 2,\n",
    "    \"skip_size\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "fuzzy_searcher = FuzzyContextSearcher(fuzzysearch_config)\n",
    "fuzzy_person_searcher = FuzzyPersonNameSearcher(fuzzysearch_config)\n",
    "\n",
    "fuzzy_searcher.index_keywords(resolution_phrases)\n",
    "fuzzy_searcher.index_spelling_variants(spelling_variants)\n",
    "#fuzzy_searcher.index_distractor_terms(distractor_terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from republic.fuzzy.fuzzy_context_searcher import FuzzyContextSearcher\n",
    "import republic.elastic.republic_elasticsearch as rep_es\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.8,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 2,\n",
    "    \"skip_size\": 2,\n",
    "    \"paragraph_index\": \"republic_paragraphs\",\n",
    "    \"paragraph_doc_type\": \"paragraph\"\n",
    "}\n",
    "\n",
    "missing_dates = [\n",
    "    {\"date_string\": \"Veneris den 5. Januarii 1725.\", \"page_start\": 11, \"page_end\": 14},\n",
    "    {\"date_string\": \"Mercuri den 10. Januarii 1725.\", \"page_start\": 21, \"page_end\": 28},\n",
    "]\n",
    "\n",
    "fuzzy_date_searcher = FuzzyContextSearcher(fuzzysearch_config)\n",
    "\n",
    "for missing_date in missing_dates:\n",
    "    fuzzy_date_searcher.index_keywords([missing_date[\"date_string\"]])\n",
    "    for page_num in range(missing_date[\"page_start\"], missing_date[\"page_end\"] + 1):\n",
    "        paragraphs = rep_es.retrieve_paragraph_by_type_page_number(es, page_num, year_config)\n",
    "        for paragraph in paragraphs:\n",
    "            if page_num == 25:\n",
    "                print(paragraph[\"text\"])\n",
    "            matches = fuzzy_date_searcher.find_candidates(paragraph[\"text\"])\n",
    "            for match in matches:\n",
    "                print(match)\n",
    "                print(\"page: {}\\tDate: {}\\tText string: {}\\n\".format(page_num, match[\"match_keyword\"], match[\"match_string\"]))\n",
    "                print(paragraph[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Paragraphs with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import republic.parser.republic_paragraph_parser as para_parser\n",
    "import republic.elastic.republic_elasticsearch as rep_es\n",
    "from republic.model.republic_phrase_model import category_index\n",
    "from republic.config.republic_config import base_config, set_config_year\n",
    "\n",
    "\n",
    "page_index = \"republic_hocr_pages\"\n",
    "page_doc_type = \"page\"\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "\n",
    "year = 1725\n",
    "data_dir = \"../../../Data/Projects/REPUBLIC/hocr/\"\n",
    "\n",
    "year_config = set_config_year(base_config, year, data_dir)\n",
    "pages_info = get_pages_info(year_config)\n",
    "\n",
    "#rep_es.delete_es_index(year_config[\"paragraph_index\"])\n",
    "\n",
    "# start on January first\n",
    "\n",
    "current_date = {\n",
    "    \"month_day\": 1,\n",
    "    \"month_name\": \"Januarii\",\n",
    "    \"month\": 1,\n",
    "    \"week_day_name\": None,\n",
    "    \"year\": year\n",
    "}\n",
    "    \n",
    "for page_id in pages_info:\n",
    "    start_scan = 1\n",
    "    end_scan = 600\n",
    "    if pages_info[page_id][\"scan_num\"] < start_scan or  pages_info[page_id][\"scan_num\"] > end_scan:\n",
    "        continue\n",
    "    page_doc = rep_es.retrieve_page_doc(es, page_id, year_config)\n",
    "    #if pages_info[page_id][\"page_type\"] != \"resolution_page\":\n",
    "    if page_doc[\"page_type\"] != \"resolution_page\":\n",
    "        continue\n",
    "    paragraphs, header = para_parser.get_resolution_page_paragraphs(page_doc)\n",
    "    print(\"page_id:\", page_id, \"\\ttype:\", page_doc[\"page_type\"], \"\\tnum columns:\", len(page_doc[\"columns\"]))\n",
    "    #print(\"num columns:\", len(page_doc[\"columns\"]), \"\\theader lines:\", [line[\"line_text\"] for line in header])\n",
    "    for paragraph_order, paragraph in enumerate(paragraphs):\n",
    "        paragraph_text = para_parser.merge_paragraph_lines(paragraph)\n",
    "        paragraph[\"metadata\"][\"categories\"] = set()\n",
    "        paragraph[\"text\"] = paragraph_text\n",
    "        paragraph[\"metadata\"][\"paragraph_num_on_page\"] = paragraph_order\n",
    "        paragraph[\"metadata\"][\"paragraph_id\"] = \"{}-para-{}\".format(page_id, paragraph_order)\n",
    "        #print(paragraph_text, \"\\n\\n\")\n",
    "        matches = fuzzy_searcher.find_candidates(paragraph_text, include_variants=True)\n",
    "        if len(matches) == 0 and para_parser.paragraph_starts_with_centered_date(paragraph):\n",
    "            print(\"DATE LINE:\", paragraph_text)\n",
    "            current_date = para_parser.extract_meeting_date(paragraph, year, current_date)\n",
    "            #print(\"paragraph_text:\", paragraph_text)\n",
    "        if para_parser.matches_participant_list(matches):\n",
    "            print(\"DAY START:\", paragraph_text)\n",
    "            #context_match = fuzzy_searcher.get_term_context(paragraph_text, match, context_size=200)\n",
    "            #print(context_match)\n",
    "            #person_matches = fuzzy_person_searcher.find_person_names_in_text(context_match[\"match_term_in_context\"])\n",
    "            if para_parser.paragraph_starts_with_centered_date(paragraph):\n",
    "                current_date = para_parser.extract_meeting_date(paragraph, year, current_date)\n",
    "                paragraph[\"metadata\"][\"categories\"].add(\"meeting_date\")\n",
    "            paragraph[\"metadata\"][\"type\"] = \"participant_list\"\n",
    "            #print(\"\\n\\tCurrent date: {}\\n\".format(current_date))\n",
    "            #person_matches = fuzzy_person_searcher.find_person_names_in_context(context_match)\n",
    "            #for person_match in person_matches:\n",
    "            #    print(\"\\t\", person_match)\n",
    "        if para_parser.matches_resolution_phrase(matches):\n",
    "            paragraph[\"metadata\"][\"type\"] = \"resolution\"\n",
    "        paragraph[\"metadata\"][\"meeting_date_info\"] = current_date\n",
    "        if current_date:\n",
    "            paragraph[\"metadata\"][\"meeting_date\"] = datetime.date(current_date[\"year\"], current_date[\"month\"], current_date[\"month_day\"])\n",
    "        paragraph[\"metadata\"][\"keyword_matches\"] = matches\n",
    "        for match in matches:\n",
    "            #print(\"\\t{}\\t{}\".format(match[\"match_keyword\"], match[\"match_string\"]))\n",
    "            if match[\"match_keyword\"] in category_index:\n",
    "                category = category_index[match[\"match_keyword\"]]\n",
    "                match[\"match_category\"] = category\n",
    "                paragraph[\"metadata\"][\"categories\"].add(category)\n",
    "                \n",
    "        #print(paragraph[\"metadata\"][\"categories\"])\n",
    "        #print(\"\\n\\n\\n\")\n",
    "        paragraph[\"metadata\"][\"categories\"] = list(paragraph[\"metadata\"][\"categories\"])\n",
    "        del paragraph[\"lines\"]\n",
    "        es.index(index=year_config[\"paragraph_index\"], doc_type=year_config[\"paragraph_doc_type\"], \n",
    "                 id=paragraph[\"metadata\"][\"paragraph_id\"], body=paragraph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
