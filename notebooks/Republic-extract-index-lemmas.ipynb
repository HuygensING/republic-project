{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Preprocessing Index Pages\n",
    "\n",
    "- filter tiny and huge text elements (i.e. deviating from average character/word width and height\n",
    "- extract page lines that are part of the main text body containing index entries\n",
    "- insert and clean up repetition symbols in index entries\n",
    "    - determine length of repetition symbol\n",
    "    - identify and replace mis-recognized repetition symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reload library is just used for developing the REPUBLIC hOCR parser \n",
    "# and can be removed once this module is stable.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# This is needed to add the repo dir to the path so jupyter\n",
    "# can load the republic modules directly from the notebooks\n",
    "import os\n",
    "import sys\n",
    "repo_dir = os.path.split(os.getcwd())[0]\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scan files: 2161\n"
     ]
    }
   ],
   "source": [
    "from republic.model.republic_hocr_model import make_hocr_page\n",
    "import republic.parser.republic_column_parser as column_parser\n",
    "from elasticsearch import Elasticsearch\n",
    "import republic.parser.republic_page_parser as page_parser\n",
    "import republic.parser.republic_paragraph_parser as paragraph_parser\n",
    "import republic.parser.republic_file_parser as file_parser\n",
    "from republic.config.republic_config import base_config, set_config_year\n",
    "import copy\n",
    "\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "year = 1725\n",
    "data_dir = \"/Users/marijnkoolen/Data/Projects/REPUBLIC/hocr\"\n",
    "\n",
    "\n",
    "def get_pages_info(config):\n",
    "    scan_files = file_parser.get_files(config[\"data_dir\"])\n",
    "    print(\"Number of scan files:\", len(scan_files))\n",
    "    return file_parser.gather_page_columns(scan_files)\n",
    "\n",
    "year_config = set_config_year(base_config, year, data_dir)\n",
    "pages_info = get_pages_info(year_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " year-1725-scan-1-odd\n",
      "skipping non-index page\n",
      "\n",
      "\n",
      " year-1725-scan-4-odd\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'column_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8813101c787f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpage_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_page_ref_lines\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_page_ref_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mcolumn_hocr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_hocr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index_entry_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_hocr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'column_id'"
     ]
    }
   ],
   "source": [
    "#from republic_index_page_parser import index_lemmata\n",
    "from collections import defaultdict\n",
    "import republic.parser.republic_index_page_parser as index_parser\n",
    "import republic.elastic.republic_elasticsearch as rep_es\n",
    "\n",
    "avg_left = 0\n",
    "lemma_index = defaultdict(list)\n",
    "curr_lemma = None\n",
    "    \n",
    "\n",
    "for page_id in pages_info:\n",
    "    page_doc = rep_es.retrieve_page_doc(es, page_id, year_config)\n",
    "    print(\"\\n\\n\", page_id)\n",
    "    if page_doc[\"page_type\"] != \"index_page\":\n",
    "        print(\"skipping non-index page\")\n",
    "        continue\n",
    "    page_doc[\"num_page_ref_lines\"] = index_parser.count_page_ref_lines(page_doc)\n",
    "    for column_info in page_doc[\"columns\"]:\n",
    "        print(\"\\n\\n\", column_info[\"column_id\"])\n",
    "        column_hocr = column_info[\"column_hocr\"]\n",
    "        lines = index_parser.get_index_entry_lines(column_hocr)\n",
    "        curr_lemma = index_parser.index_lemmata(column_info[\"column_id\"], lines, lemma_index, curr_lemma)\n",
    "        print(\"returned lemma:\", curr_lemma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Searching of Keywords in the Resolutions\n",
    "\n",
    "Knowing which keywords should appear in the text, possibly with some spelling variation and OCR errors, we can use a fuzzy search algorithm to find candidate matches. \n",
    "\n",
    "Keywords that are similar to each other are registered as distractor terms, so matches are assigned as candidates to the nearest of sets of similar keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a4dc8454c86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrepublic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepublic_hocr_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_hocr_page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlemma_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "from republic.parser.republic_base_page_parser import merge_text_lines\n",
    "from republic.model.republic_hocr_model import make_hocr_page\n",
    "\n",
    "lemma_matches = defaultdict(list)\n",
    "\n",
    "def add_context(match, page_text):\n",
    "    context = fuzzy_searcher.get_term_context(page_text, match, context_size=40)\n",
    "    match[\"match_term_in_context\"] = context[\"match_term_in_context\"]\n",
    "    match[\"context_start_offset\"] = context[\"start_offset\"]\n",
    "    match[\"context_end_offset\"] = context[\"end_offset\"]\n",
    "\n",
    "for scan_file in scan_files:\n",
    "    resolution_page_num = scan_file[\"scan_page_num\"] - 90\n",
    "    if scan_file[\"scan_page_num\"] <= 90:\n",
    "        continue\n",
    "    print(scan_file[\"scan_page_num\"], resolution_page_num)\n",
    "    hocr_page = make_hocr_page(scan_file)\n",
    "    page_text = merge_text_lines(hocr_page)\n",
    "    matches = fuzzy_searcher.find_candidates(page_text)\n",
    "    for match in matches:\n",
    "        lemma_matches[match[\"match_keyword\"]] += [match]\n",
    "        add_context(match, page_text)\n",
    "        match[\"page_num\"] = scan_file[\"scan_page_num\"]\n",
    "        print(match[\"match_keyword\"], \"\\t\", match)\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6960c3442893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\tAantal kandidaten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemma_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tKandidaat:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"match_string\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tPagina:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemma_matches' is not defined"
     ]
    }
   ],
   "source": [
    "for lemma in sorted(lemma_matches):\n",
    "    print(\"\\n\", lemma, \"\\tAantal kandidaten:\", len(lemma_matches[lemma]), \"\\n\")\n",
    "    for match in lemma_matches[lemma]:\n",
    "        print(\"\\tKandidaat:\", match[\"match_string\"])\n",
    "        print(\"\\tPagina:\", match[\"page_num\"])\n",
    "        print(\"\\tContext:\", match[\"match_term_in_context\"][5:-5])\n",
    "        print()\n",
    "\n",
    "\n",
    "for lemma in lemma_index:\n",
    "    print(\"\\nTrefwoord:\", lemma)\n",
    "    #print(lemma_index[lemma])\n",
    "    for entry in lemma_index[lemma]:\n",
    "        pages = \", \".join([str(page_ref) for page_ref in entry[\"page_refs\"]])\n",
    "        description = entry[\"description\"][:70]\n",
    "        print(\"\\tPagina:\", pages, \"\\tBeschrijving:\", description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan 45 uneven is first resolution page\n",
    "# page num: 91\n",
    "\n",
    "from fuzzy_context_searcher import FuzzyContextSearcher\n",
    "import pandas as pd\n",
    "\n",
    "config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.8,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 3,\n",
    "    \"skip_size\": 0,\n",
    "}\n",
    "\n",
    "fuzzy_searcher = FuzzyContextSearcher(config)\n",
    "\n",
    "keywords = [\n",
    "    \"Admiraliteyt tot Amfterdam\", \n",
    "    \"Admiraliteyt in het Noorder Quartier\", \n",
    "    \"Admiraliteyt in Vrieslandt\", \n",
    "    \"Admiralteyt in Zeelandt\",\n",
    "    \"Varckens\"\n",
    "]\n",
    "\n",
    "distractor_terms = {\n",
    "    \"Admiraliteyt tot Amfterdam\": {\n",
    "        \"Admiraliteyt in het Noorder Quartier\", \"Admiraliteyt in Vrieslandt\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiraliteyt in het Noorder Quartier\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in Vrieslandt\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiraliteyt in Vrieslandt\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in het Noorder Quartier\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiralteyt in Zeelandt\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in het Noorder Quartier\", \"Admiraliteyt in Vrieslandt\"\n",
    "    },\n",
    "}\n",
    "fuzzy_searcher.index_keywords(keywords)\n",
    "fuzzy_searcher.index_distractor_terms(distractor_terms)\n",
    "\n",
    "hocr_resolution_pages = []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
