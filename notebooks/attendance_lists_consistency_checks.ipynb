{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Attendance lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for consolidating the attendancelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reload library is just used for developing the REPUBLIC hOCR parser \n",
    "# and can be removed once this module is stable.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#This is needed to add the repo dir to the path so jupyter\n",
    "# can load the republic modules directly from the notebooks\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# from collections import defaultdict\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "# import numpy as np\n",
    "# #import xmltodict\n",
    "# import networkx as nx\n",
    "# from sqlalchemy import text\n",
    "\n",
    "\n",
    "repo_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -U jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repress the warnings, esp. from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the attendancelists for one year\n",
    "\n",
    "So we have something to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_attendancelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging to attendancelist.log\n"
     ]
    }
   ],
   "source": [
    "runner1730 = run_attendancelist.RunAll(year=1730)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. finding presidents\n",
      "288 found\n",
      "2.find provincial extraordinaris gedeputeerden\n",
      "CPU times: user 1min 3s, sys: 319 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%time runner1730.initial_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. finding unmarked text\n",
      "4. joining presidents and delegates\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "Phrase(, )\n",
      "total 116 found \n",
      "CPU times: user 3min 19s, sys: 1.32 s, total: 3min 20s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%time runner1730.gather_found_delegates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time runner1728.verify_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchobs = runner1728.searchobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "all_marked_names = Counter()\n",
    "all_marked_presidents = Counter()\n",
    "vanders = defaultdict(list)\n",
    "c = 0\n",
    "for ob in searchobs:\n",
    "    so = searchobs[ob].matched_text\n",
    "    for s in so.spans:\n",
    "        if s.type in ['delegate', 'president']:\n",
    "            if s.pattern == '':\n",
    "                pattern = s.pattern\n",
    "            else:\n",
    "                pattern = so.item[s.begin:s.end]\n",
    "            all_marked_names.update([pattern])\n",
    "            if s.pattern not in all_marked_names:\n",
    "                if s.pattern not in ['']:\n",
    "                    if s.pattern.lower() in ['van', 'vander']:\n",
    "                        vanders[ob].append(s)\n",
    "            if s.type in ['president']:\n",
    "                all_marked_presidents.update([pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner1730.find_unmarked_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_attendancelist import FuzzyKeywordGrouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_names = FuzzyKeywordGrouper(list([key for key in all_marked_names.keys() if key != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "ll = list(chain(grouped_names.distance_list.keys(), grouped_names.distance_list.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftouts = [name for name in all_marked_names.keys() if  name not in ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vtg = grouped_names.vars2graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "best_matches = {}\n",
    "for group in vtg:\n",
    "    i = argmax([all_marked_names[i] for i in group])\n",
    "    best_matches[group[i]] = [g for g in group if g != group[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_prezs = FuzzyKeywordGrouper(list([key for key in all_marked_presidents.keys() if key != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pvtg = grouped_prezs.vars2graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "best_match_pres = {}\n",
    "for group in pvtg:\n",
    "    i = argmax([all_marked_presidents[i] for i in group])\n",
    "    best_match_pres[group[i]] = [g for g in group if g != group[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "expandables = defaultdict(list)\n",
    "for k1 in best_match_pres.keys():\n",
    "    for k2 in best_matches.keys():\n",
    "        x1, x2 = sorted([k1,k2], key=lambda z: len(z))\n",
    "        if x1.lower() in x2.lower() and x1!=x2:\n",
    "            expandables[x1].append(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now have to merge them\n",
    "expandables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_for_delete = []\n",
    "for i in best_matches.keys():\n",
    "    if len(i.split(' ')) < 2 and str.islower(i[0]):\n",
    "            mark_for_delete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_for_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs(seq):\n",
    "    result = []\n",
    "    for i in enumerate(seq):\n",
    "        if i[0] < len(seq)-1:\n",
    "            result.append((i[1], seq[i[0]+1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductible = []\n",
    "for pair in pairs(list(best_matches)):\n",
    "    if pair[0] not in mark_for_delete:\n",
    "        npair = sorted(pair, key=lambda x: len(x))\n",
    "        if npair[0].lower() in npair[1].lower():\n",
    "            reductible.append(pair)\n",
    "reductible"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nms = []\n",
    "for s in searchobs[ob].matched_text.spans:\n",
    "        if s not in nms:\n",
    "#     if len(s.pattern) < len(s.delegate_name):\n",
    "            nms.append(s.pattern)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenst_vals = run_attendancelist.levenst_vals\n",
    "#runner1730.framed_gtlm.variants.apply(lambda x: levenst_vals(x, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = []\n",
    "for k, v in best_matches.items():\n",
    "    if k not in mark_for_delete:\n",
    "        if k in expandables:\n",
    "            print (k)\n",
    "        # we substitute the key for the longer pattern\n",
    "            k = expandables[k][0]\n",
    "        \n",
    "        phrase = {'phrase': k, 'label':k, 'variants': v} \n",
    "        variants.append(phrase)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "all_phrases = [v['phrase'] for v in variants]\n",
    "found = runner1730.framed_gtlm\n",
    "for phrase in all_phrases:\n",
    "    match = found.variants.apply(lambda x: levenst_vals(x, phrase))\n",
    "    print(phrase, list(found.loc[match].name)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanpat = re.compile('vande[rn]?|([dt]+e?[rn]?)\\s+|van', re.I)\n",
    "for phrase in all_phrases:\n",
    "    print(phrase, [x.strip() for x in vanpat.split(phrase) if x and x.strip() !=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_namepart(variant, found=found):\n",
    "    result = {}\n",
    "    result['phrase'] = variant\n",
    "    result['contained'] = False\n",
    "    result['exactname'] = False\n",
    "    match = found.variants.apply(lambda x: levenst_vals(x, phrase))\n",
    "    matchname = list(found.loc[match].name)[0]\n",
    "    refid = list(found.loc[match].ref_id)[0]\n",
    "    result['matchname'] = matchname\n",
    "    result['id'] = refid\n",
    "    searchpat = [x.strip() for x in vanpat.split(phrase) if x and x.strip() !='']\n",
    "    for p in searchpat:\n",
    "        if p in matchname:\n",
    "            result['exactname'] = True\n",
    "            if len(phrase)<len(matchname):\n",
    "                result['contained'] = True\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = [v['phrase'] for v in variants]\n",
    "found = runner1728.framed_gtlm\n",
    "matches = []\n",
    "for phrase in all_phrases:\n",
    "    matches.append(is_namepart(phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "groupedmatches = defaultdict(list)\n",
    "for item in matches:\n",
    "    groupedmatches[item['id']].append(item)\n",
    "groupedmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy_search.fuzzy_phrase_searcher import FuzzyPhraseSearcher\n",
    "from fuzzy_search.fuzzy_phrase_model import PhraseModel\n",
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.5,\n",
    "    \"ignorecase\": True,\n",
    "    \"ngram_size\": 3,\n",
    "    \"skip_size\": 1,\n",
    "}\n",
    "fs = FuzzyPhraseSearcher(fuzzysearch_config)\n",
    "model = PhraseModel(variants, config=fuzzysearch_config)\n",
    "fs.index_phrase_model(phrase_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "obs = choices(list(searchobs.keys()), k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mappings = run_attendancelist.reverse_dict(best_match_pres)\n",
    "name_mappings.update(run_attendancelist.reverse_dict(best_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "for item in name_mappings:\n",
    "    match = max(fs.find_matches(item, include_variants=True), key=lambda x: x.levenshtein_similarity)\n",
    "    print(item, match.label, match.offset)\n",
    "    p = ''.join(('(?:', item, '){e}'))\n",
    "    peind = regex.compile(p, flags=regex.BESTMATCH)\n",
    "    r = peind.search(match.label)\n",
    "    print(r,'====\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ob in searchobs:\n",
    "    so = searchobs[ob].matched_text\n",
    "    for span in enumerate(so.spans):\n",
    "        s = span[1]\n",
    "        if s.type in ['president','delegate']:\n",
    "    #         pres = so.spans[span[0]-1]\n",
    "    #         try:\n",
    "    #             post = so.spans[span[0]+1]\n",
    "    #         except IndexError:\n",
    "    #             post = None\n",
    "    #         m = fs.find_matches(s.pattern, include_variants=True)\n",
    "            pattern = so.item[s.begin:s.end]\n",
    "            longpat = name_mappings.get(pattern)\n",
    "#             if longpat and longpat!='':\n",
    "#                 \n",
    "#                 pattern, name_mappings[pattern]\n",
    "            if not longpat:\n",
    "                try:\n",
    "                    longpats = max(fs.find_matches(pattern, include_variants=True), key=lambda x: x.levenshtein_similarity)\n",
    "                    longpat = longpats.label\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            if longpat and longpat!='':\n",
    "                p = ''.join(('(?:', longpat, '){e}'))\n",
    "                peind = regex.compile(p, flags=regex.BESTMATCH)\n",
    "                r = peind.search(so.item)\n",
    "                begin = longpat.find(pattern)\n",
    "                so.set_span(span=r.span(),\n",
    "                           clas='delegate',\n",
    "                           delegate_name=s.delegate_name, \n",
    "                           score = s.delegate_score)\n",
    "    #             b = s.begin - phrase.offset\n",
    "    #             e = b + len(s.delegate_name)\n",
    "    #             mt = so.item[b:e]\n",
    "    #             sc = score_levenshtein_distance_ratio(s.delegate_name, mt)\n",
    "    #             overlap = []\n",
    "    #             if sc > 0.5:\n",
    "    #                 if b < pres.end:\n",
    "    #                     overlap.append( pres.pattern)\n",
    "    #                 if post:\n",
    "    #                     if e > post.begin:\n",
    "    #                         overlap.append(post.pattern)\n",
    "    #                 so.set_span(span=(b,e),\n",
    "    #                             clas='delegate',\n",
    "    #                             delegate_name=s.delegate_name, \n",
    "    #                             score = s.delegate_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitems = ';'.join(so.get_unmatched_text())\n",
    "items = re.split('[;,\\.]+', jitems)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = run_attendancelist.found_delegates\n",
    "df = run_attendancelist.abbreviated_delegates\n",
    "i_s = run_attendancelist.iterative_search\n",
    "noresults = Counter()\n",
    "for ob in searchobs:\n",
    "    so = searchobs[ob].matched_text\n",
    "    jitems = ';'.join(so.get_unmatched_text())\n",
    "    items = [item.strip() for item in re.split('[;,\\.]+', jitems) if item.strip()!='' and len(item.strip())>2]\n",
    "    for item in items:\n",
    "        b = e = delegatename = score = None\n",
    "#        r = fs.find_matches(item, include_variants=True)\n",
    "        try:\n",
    "            longpats = max(fs.find_matches(item, include_variants=True), key=lambda x: x.levenshtein_similarity)\n",
    "            if getattr(longpats, 'string')!='':\n",
    "                longpat = longpats.string\n",
    "                b = longpats.offset\n",
    "                e = b + len(longpats.string)\n",
    "                delegatename = longpats.label\n",
    "                score = longpats.levenshtein_similarity\n",
    "                # print(longpats)\n",
    "            else:\n",
    "                match = found.variants.apply(lambda x: levenst_vals(x, item))\n",
    "                print(item, list(found.loc[match].name)[0])\n",
    "        except ValueError:\n",
    "            # try:\n",
    "            #     match = found.variants.apply(lambda x: levenst_vals(x, item))\n",
    "            #     b = so.item.find(item)\n",
    "            #     e = b + len(item)\n",
    "            #     delegatename = list(found.loc[match].name)[0]\n",
    "            #     score = 0\n",
    "            # except IndexError:\n",
    "            #     try:\n",
    "            #         match = i_s(name=item, year=1730, debug=False, df=df)\n",
    "            #         if len(match)>0:\n",
    "            #             b = so.item.find(item)\n",
    "            #             e = b + len(item)\n",
    "            #             delegatename = list(match.name)[0]\n",
    "            #             score = 0\n",
    "            #     except IndexError:\n",
    "                    noresults.update([item])\n",
    "        # if b:\n",
    "        #     so.set_span(span=(b,e),\n",
    "        #     clas='delegate',\n",
    "        #     delegate_name=delegatename,\n",
    "        #     score = score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fs.phrase_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "item = 'Wijnbergen'\n",
    "match = found.variants.apply(lambda x: levenst_vals(x, item))\n",
    "            #     b = so.item.find(item)\n",
    "            #     e = b + len(item)\n",
    "found.loc[match].name\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for identifying unknown spans\n",
    "\n",
    "- run regular checks\n",
    "- gather unmarked snippets\n",
    "- group them, with references to days (how?)\n",
    "- gather most frequent non-identified groups\n",
    "    - can we identify them?\n",
    "    - if we can, apply identification to references\n",
    "    - if not, make unidentified references (which id?) and add it to the list of references\n",
    "- how many unidentified spans have we still got left\n",
    "- filter out spurious words (frequency?)\n",
    "- how many are there still. Do they look like names? (begin with 'van' pat, capitals, are they part of composite names, that we have identified)\n",
    "\n",
    "further processing:\n",
    "\n",
    "- do we have double identification that may be reduced, because they are part of names\n",
    "- after that:\n",
    "    - are there any outlying presidents: that occur only once or out of place (in a differently identified row)\n",
    "    - can we do something with the order of delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make an object that points at attendancelists. \n",
    "# Note that for use in comparisons that involve typechecking for strings\n",
    "# we may have to take a roundtrip converting it to a real string and later retrieve the actual object, but that's fine\n",
    "\n",
    "from collections import UserString\n",
    "class StringWithContext(UserString):\n",
    "    def __init__(self, item='', reference=None):\n",
    "        self.data = item\n",
    "        self.references = []\n",
    "        \n",
    "    def set_reference(self, reference):\n",
    "        self.references.append(reference)\n",
    "        \n",
    "    def get_references(self):\n",
    "        return self.references\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "x = StringWithContext('bla')\n",
    "type(x)\n",
    "x.lower()\n",
    "x.set_reference('q23')\n",
    "'q23' in x.references\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather unmarked snippets\n",
    "# group them, with references to days by using the stringwithcontext object\n",
    "\n",
    "found = run_attendancelist.found_delegates\n",
    "df = run_attendancelist.abbreviated_delegates\n",
    "i_s = run_attendancelist.iterative_search\n",
    "noresults = []\n",
    "\n",
    "for ob in searchobs:\n",
    "    so = searchobs[ob].matched_text\n",
    "    jitems = ';'.join(so.get_unmatched_text())\n",
    "    items = [item.strip() for item in re.split('[;,\\.]+', jitems) if item.strip()!='' and len(item.strip())>2]\n",
    "    for item in items:\n",
    "        contextitem = StringWithContext(item)\n",
    "        contextitem.set_reference(so)\n",
    "        noresults.append(contextitem)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#these are unmarked spans in the attendance lists\n",
    "#probably we best sort these by hand and try and assign them \n",
    "# Counter([i for i in noresults]).most_common()\n",
    "noresultscounter = Counter(noresults)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "noresults[-1].get_references()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gather most frequent non-identified groups\n",
    "\n",
    "from republic.helper.utils import best_match\n",
    "unmatched_grouped = FuzzyKeywordGrouper([str(k) for k in noresultscounter.keys()]).vars2graph()\n",
    "consolidated_groups = {}\n",
    "for group in unmatched_grouped:\n",
    "    i = argmax([noresultscounter[i] for i in group])\n",
    "    consolidated_groups[group[i]] = [g for g in group if g != group[i]]\n",
    "        \n",
    "consolidated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgroups = {}\n",
    "for g in consolidated_groups:\n",
    "    ks = [g] + consolidated_groups[g]\n",
    "    r = list(filter(lambda i: i in ks, noresults))\n",
    "    xgroups[g] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = run_attendancelist.junksweeper\n",
    "suspects = []\n",
    "for key in xgroups:\n",
    "    jmatches = js.find_matches(key, include_variants=True, use_word_boundaries=False)\n",
    "    if len(jmatches)>0:\n",
    "        bestjmatch = max(jmatches, key=lambda m: m.levenshtein_similarity)\n",
    "        if not vanpat.search(bestjmatch.string):\n",
    "            suspects.append(key)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_from_dict(pattern, searcher=fs, founddb=found):\n",
    "    \"\"\"pattern should be string in context\"\"\"\n",
    "    \n",
    "    longpats = fs.find_matches(pattern, include_variants=True)\n",
    "    if len(longpats)>0:\n",
    "        result = max(longpats, key=lambda x: x.levenshtein_similarity)\n",
    "    else:\n",
    "        match = found.variants.apply(lambda x: levenst_vals(x, item))\n",
    "        result = list(found.loc[match].name)\n",
    "        if len(result)==0:\n",
    "            result = pattern\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_span(keyw):\n",
    "    for p in xgroups[keyw]:\n",
    "        obs = p.get_references()\n",
    "        for ob in obs:\n",
    "            s = re.search(re.escape(str(p)), ob.item)\n",
    "            print(ob.item[:s.span()[0]], \"##\", p, \"##\", ob.item[s.span()[1]:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_from_dict('de Raadt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in xgroups:\n",
    "    if key not in suspects:\n",
    "        r = match_from_dict(key)\n",
    "        print(key)\n",
    "        if r:\n",
    "            make_span(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_span('van Cattenburgh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     can we identify them?\n",
    "#     if we can, apply identification to references\n",
    "#     if not, make unidentified references (which id?) and add it to the list of references\n",
    "\n",
    "for key in xgroups:\n",
    "    if key not in suspects:\n",
    "        # print('\\n\\n=======\\n', key) \n",
    "        splt = re.split(vanpat, key)\n",
    "        if len(splt)>1:\n",
    "            print(splt[-1])\n",
    "            print(fs.find_matches(splt[-1], include_variants=True))\n",
    "        # print(is_namepart(key))\n",
    "            match = found.variants.apply(lambda x: levenst_vals(x, key))\n",
    "            print(list(found.loc[match].name))\n",
    "            if len(match)==0:\n",
    "                match = i_s(name=item, year=1730, debug=False, df=df)\n",
    "                if len(match)>0:\n",
    "                    print(match[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = run_attendancelist.found_delegates\n",
    "df = run_attendancelist.abbreviated_delegates\n",
    "i_s = run_attendancelist.iterative_search\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rs = [set(range(r.offset, r.offset + len(r.string))) for r in res if r.label=='heere']\n",
    "result = []\n",
    "for i in pairs(rs):\n",
    "    if i[0].isdisjoint(i[1]):\n",
    "        result += [i[0], i[1]]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = searchobs['session-1730-01-03-num-1'].matched_text\n",
    "m.get_fragments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.5,\n",
    "    \"ignorecase\": True,\n",
    "    \"ngram_size\": 3,\n",
    "    \"skip_size\": 1,\n",
    "}\n",
    "import random\n",
    "from republic.analyser.attendance_lists.pattern_finders import make_province_searcher\n",
    "ps = make_province_searcher(config=fuzzysearch_config)\n",
    "ks = random.choices(list(runner1730.searchobs.keys()), k=20)\n",
    "# pres = []\n",
    "# prs = []\n",
    "for T in ks:\n",
    "    ob = runner1730.searchobs[T].matched_text\n",
    "    print(T, ob.get_fragments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchobs = runner1730.searchobs\n",
    "from IPython.display import HTML\n",
    "htmlout=[]\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\\n<h4>{T}</h4>\\n\" + rest\n",
    "    if url:\n",
    "        rest += f\"\"\"<br/><br/><a href='{url}'>link naar {T}-image</a><br/>\"\"\"\n",
    "    htmlout.append(rest)\n",
    "#out.reverse()\n",
    "HTML(\"<br><hr><br>\".join(htmlout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in ks:\n",
    "    ob = runner1730.searchobs[T].matched_text\n",
    "    txt = ob.item\n",
    "    res = ps.find_matches(text=txt, include_variants=True)\n",
    "    print (T,txt, [r for r in res if r.label=='heere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "t_out=[]\n",
    "searchobs = runner1730.searchobs\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\"\"\\n<tr><td><strong>{T}</strong></td><td>{rest}</td>\"\"\"\n",
    "#     if url:\n",
    "#         rest += f\"\"\"<td><a href='{url}'>link naar {T}-image</a></td>\"\"\"\n",
    "    rest += \"</tr>\"\n",
    "    t_out.append(rest)\n",
    "#out.reverse()\n",
    "outtable = \"\".join(t_out)\n",
    "#HTML(f\"<table>{outtable}</table>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = runner1730.year\n",
    "with open(f\"/Users/rikhoekstra/Downloads/{17}_sample_nocheck.html\", 'w') as flout:\n",
    "    flout.write(f\"<html><body><h1>results for {year}</h1>\\n\")\n",
    "    flout.write(f\"<table>{outtable}</table>\")\n",
    "    flout.write(\"</body></html>\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This could be used for further checking\n",
    "runner1730.find_unmarked_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = run_attendancelist.make_province_searcher(config=fuzzysearch_config)\n",
    "i = searchobs[obs[5]].matched_text.item\n",
    "ps.find_matches(i, include_variants=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely the verification code needs an overhaul, but we'll see about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: find delegates in resolutions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# placeholder : query for resolutions from 1730\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\"range\": {\n",
    "            \"metadata.meeting_date\": \n",
    "            {\"gt\": \"1730-01-01\",\n",
    "             \"lt\": \"1730-12-31\"}\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remediate text length\n",
    "\n",
    "Sometimes, there are faulty attendancelists, that contain parts of the following resolutions. Further on, there is more finegrained code, but here we cutdown the size of the texts on basis of the mean text length, as there is not really an attendancelist that should be much longer than other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presidents code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner1730.presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchobs = runner1730.searchobs\n",
    "met_en_zonder = {}\n",
    "met_pr = {}\n",
    "zonder_pr = {}\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    pr = [s for s in ob.spans if s.type == 'president']\n",
    "    if len(pr)==0:\n",
    "        zonder_pr[T]=pr\n",
    "    else:\n",
    "        met_pr[T]=pr\n",
    "met_en_zonder['met_pr'] = met_pr\n",
    "met_en_zonder['zonder_pr'] = zonder_pr\n",
    "met_en_zonder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate some statistics for these presidents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "starts = [v[0].begin for v in met_en_zonder['met_pr'].values()]\n",
    "ends = [v[0].end for v in met_en_zonder['met_pr'].values()]\n",
    "lengths = [v[0].end - v[0].begin for v in met_en_zonder['met_pr'].values()]\n",
    "print(\"mean beginning and end offsets:\")\n",
    "meanstart = round(statistics.mean(starts),0)\n",
    "print(meanstart)\n",
    "meanend = round(statistics.mean(ends),0)\n",
    "print(meanend)\n",
    "print(\"min and max start offsets:\")\n",
    "print(\"min: \", min(starts),\";max: \", max(starts))\n",
    "print(\"min and max end offsets:\")\n",
    "print(\"min: \", min(ends), \";max: \", max(ends))\n",
    "print(\"try and find some outlying values\")\n",
    "print(\"mean: \", round(statistics.mean(lengths), 0))\n",
    "minlength = min(lengths)\n",
    "maxlength = max(lengths)\n",
    "minpres = [v[0].pattern for k,v in met_en_zonder['met_pr'].items() if v[0].end-v[0].begin==minlength ]\n",
    "print(\"min: \", minlength , \"max: \", maxlength)\n",
    "#we should do something more with this, but at this point we cannot easily retrieve the spans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare try and complete the list of presidents by 1) finding the sessions without president and see if we can find any of them in the first part of the text. This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DelegateChecker(keywords=keywords, searchobs=runner1730.searchobs)\n",
    "r = dg.searcher.find_matches('den heere van Isselmuden')[0]\n",
    "r.phrase\n",
    "r.levenshtein_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not generalize much, but we'll leave that for later\n",
    "from collections import defaultdict\n",
    "from republic.analyser.attendance_lists.searchers import make_herensearcher\n",
    "from republic.fuzzy.fuzzy_keyword_searcher import score_levenshtein_distance_ratio\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "class DelegateChecker(object):\n",
    "    def __init__(self, keywords, searchobs):\n",
    "        self.searcher = make_herensearcher(keywords=keywords)\n",
    "        self.context_searcher = make_herensearcher(keywords={'PRAESENTIBUS':[], 'Den Heere':[]})\n",
    "        self.searchobs = searchobs\n",
    "        self.likely_prez = defaultdict(list)\n",
    "        self.checkdeps = self.make_noprez()\n",
    "        self.ks = self.make_keys()\n",
    "        \n",
    "    def make_keys(self):\n",
    "        ks = list(self.checkdeps.keys())\n",
    "        ks.sort() # we depend on the key order\n",
    "        return ks\n",
    "        \n",
    "    def get_vicinity_presidents(self,t):\n",
    "        ti = self.ks.index(t)\n",
    "        mn = max(ti-7, 0) \n",
    "        mx = min(ti+7, len(self.ks)-1)\n",
    "        likely_prez = defaultdict(list)\n",
    "        for pt in self.ks[mn:mx]:\n",
    "            if self.checkdeps[pt]:\n",
    "                president = self.checkdeps[pt][0]\n",
    "                name = president.delegate_name\n",
    "                pattern = president.pattern\n",
    "                likely_prez[name].append(pattern)\n",
    "        return likely_prez\n",
    "        # for p in likely_prez:\n",
    "        #     ra.all_matched\n",
    "#         xsisting_kws = list(self.searcher.keyword_index.keys())\n",
    "#         kws = [k for k in list(likely_prez.keys()) if k not in xsisting_kws]            \n",
    "#         self.searcher.index_keywords(kws)\n",
    "#         for k in kws:\n",
    "#             for variant in likely_prez[k]:\n",
    "#                 self.searcher.index_spelling_variant(k, variant=variant)\n",
    "    \n",
    "    def check(self):\n",
    "        results = []\n",
    "        chosen = None\n",
    "        for t in self.ks:\n",
    "            start = 0\n",
    "            if self.checkdeps[t] is None:\n",
    "                txt = self.searchobs[t].text\n",
    "                if len(txt) > 30:\n",
    "                    candidate = None\n",
    "                    res = self.searcher.find_matches(text=txt, include_variants=True)\n",
    "#                     if len(res) == 0:\n",
    "#                         print('breaking', t)\n",
    "#                         break\n",
    "#                     hres = [r for r in res if r.label=='Den Heere']\n",
    "#                     if hres:\n",
    "#                         sres = min(hres, key=lambda x: x.offset)\n",
    "#                         if len(sres) > 0:\n",
    "#                             start = sres.offset  + len(sres.string)       \n",
    "#                     pres = [r for r in res if r.label=='PRAESENTIBUS']\n",
    "#                     if len(pres) > 0:\n",
    "#                         eres = min(pres, key=lambda x: x._offset)\n",
    "#                         end = eres.offset\n",
    "#                     else:\n",
    "#                         end = start + 20\n",
    "#                     stxt = txt[start:end]\n",
    "#                     stxt = stxt.strip()\n",
    "                    likely_prez = self.get_vicinity_presidents(t)\n",
    "                    mxs = []\n",
    "                    for i in res:\n",
    "                        lmax = []\n",
    "                        for p in likely_prez:\n",
    "                            lmax.append(max([score_levenshtein_distance_ratio(i.string, v) for v in likely_prez[p]]))\n",
    "                        l = argmax(lmax)\n",
    "                        mxs.append(l)\n",
    "                    chosen = argmax(mxs)\n",
    "                    score = mxs[chosen]\n",
    "#                    print(score, chosen, len(res), t)\n",
    "                    if score > 0.8:\n",
    "    #                     for cand in likely_prez:\n",
    "    #                         candidate = max([(i, score_levenshtein_distance_ratio(stxt, i)) for i in likely_prez[cand]])\n",
    "                        try:\n",
    "                            candidate = res[chosen]\n",
    "                        except IndexError:\n",
    "                            print(chosen, len(res), t)\n",
    "#                     else:\n",
    "#                         prop = self.searcher.find_matches(stxt, include_variants=True)\n",
    "#                         if len(prop)>0:\n",
    "#                             candidate = max(prop, key=lambda x: x.levenshtein_similarity)\n",
    "#                         else:\n",
    "#                             candidate = None\n",
    "                    if candidate:\n",
    "                        b = candidate.offset\n",
    "                        e = b + len(candidate.string)\n",
    "                        etxt = txt[:b], txt[b:e], txt[e:]\n",
    "                    else:\n",
    "                        etxt = txt\n",
    "                    results.append([t, etxt]) \n",
    "        return results\n",
    "\n",
    "    \n",
    "    def make_noprez(self):\n",
    "        noprez = {}\n",
    "        for T in self.searchobs:\n",
    "            ob = self.searchobs[T]\n",
    "            p = [s for s in ob.get_spans() if s.type=='president']\n",
    "            if p:\n",
    "                noprez[T] = p\n",
    "            else:\n",
    "                noprez[T] = None\n",
    "        return noprez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {i.name:[v.form for v in i.variants['general']] for i in runner1730.moregentlemen}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywrds = {}\n",
    "for e in runner1730.all_matched:\n",
    "    i = runner1730.all_matched[e]\n",
    "    nm = i[\"name\"]\n",
    "    f = [i[\"m_kw\"]]\n",
    "    f.extend([v[1] for v in i[\"variants\"]])\n",
    "    keywrds[nm]=list(set(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keywrds['Tamminga van Alberda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlgchk = DelegateChecker(keywords=keywrds, searchobs=runner1730.searchobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchobs = dlgchk.searchobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srch = dlgchk.searcher\n",
    "zscore = []\n",
    "for t in dlgchk.ks:\n",
    "    if dlgchk.checkdeps[t] is None:\n",
    "        sessie = searchobs[t]\n",
    "        txt = sessie.text\n",
    "        res = srch.find_matches(txt, include_variants=True)\n",
    "        likely_prez = dlgchk.get_vicinity_presidents(t)\n",
    "        mxs = []\n",
    "        for i in res:\n",
    "            lmax = []\n",
    "            for p in likely_prez:\n",
    "                lmax.append(max([score_levenshtein_distance_ratio(i.string, v) for v in likely_prez[p]]))\n",
    "            l = argmax(lmax)\n",
    "            mxs.append(l)\n",
    "        if len(mxs) > 0:\n",
    "            chosen = argmax(mxs)\n",
    "            score = mxs[chosen]\n",
    "            omax = min(res, key=lambda x: x.offset)\n",
    "            zscore.append([t, res[chosen], omax] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try and fill up spaces in the spans\n",
    "\n",
    "We had to find delegates by searching for their names. But sometimes names that appear in the database ('the canonical name') differs from the one we find in the actual text. Leaving aside OCR errors, this has different forms:\n",
    "\n",
    "- the actual spelling of the name is different (_ten Brincke_ vs _ten Brinke_; _Isselmude_ vs _Ysselmuden_), but there are no rules as to whether the database or the text spelling is more archaic.\n",
    "- the name in the text designates the territorial name instead of the family name in the database. For instance: _Noortwijk_ vs _van Aerssen_.\n",
    "- the name consists of different parts. Because of the matching and tokenization and OCR-errors (especially in separation characters such as . (period) or , (comma)) we could mostly only match one name part. This leaves out many prepositions common in Dutch names (like _van_, _ter_ etc). For example: _Bors_ van Waveren, Taats van _Amerongen_, though in the last case often both parts are matched separately: _Taats_ van _Amerongen_\n",
    "- the name in the database consists of more or different parts than the name in the text. This may go any way _van Heeckeren_ vs _van Heeckeren van de Brantzenburg_ (this is an especially interesting case, as there are sometimes two _van Heeckeren_ at the same time. Sometimes different parts of the name have been resolved to different delegates, for instance _de Milan Visconti_: _Visconti_ to _de Milan Visconti_ and _Milan_ to _Becker_\n",
    "\n",
    "Proposed solution:\n",
    "- try and find the actual occurrence of the names by comparing database and text and by filling out the recognized patterns with surrounding words (that have not been marked or resolve to the same delegate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first find spans and check for adjacency\n",
    "searchobs = runner1730.searchobs\n",
    "allspans = {}\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    allspans[T] = {\"afwijkingen\":spns2gaps(o), \"text\":ob.item}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs =['session-1730-02-22-num-1',\n",
    " 'session-1730-09-12-num-1',\n",
    " 'session-1730-09-06-num-1',\n",
    " 'session-1730-08-04-num-1',\n",
    " 'session-1730-10-04-num-1',\n",
    " 'session-1730-04-04-num-1',\n",
    " 'session-1730-03-07-num-1',\n",
    " 'session-1730-11-07-num-1',\n",
    " 'session-1730-01-27-num-1',\n",
    " 'session-1730-10-19-num-1',\n",
    " 'session-1730-09-30-num-1',\n",
    " 'session-1730-03-27-num-1',\n",
    " 'session-1730-05-17-num-1',\n",
    " 'session-1730-11-24-num-1',\n",
    " 'session-1730-07-21-num-1',\n",
    " 'session-1730-12-22-num-1',\n",
    " 'session-1730-07-03-num-1',\n",
    " 'session-1730-05-27-num-1',\n",
    " 'session-1730-03-07-num-1',\n",
    " 'session-1730-02-07-num-1',\n",
    " 'session-1730-02-22-num-1',\n",
    " 'session-1730-03-24-num-1',\n",
    " 'session-1730-11-29-num-1',\n",
    " 'session-1730-07-08-num-1',\n",
    " 'session-1730-12-13-num-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup objects for testing\n",
    "from random import choices\n",
    "obs = choices(list(allspans.keys()), k=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "def spns2gaps(ob):\n",
    "    o = ob.spans\n",
    "    \n",
    "    afwijkingen = defaultdict(dict)\n",
    "    if o[0].end == 0:\n",
    "        afwijkingen[o[0]]['trailer'] = (o[0].end, o[1].begin)\n",
    "    for pair in pairs(o):\n",
    "        s1 = pair[0]\n",
    "        s2 = pair[1]\n",
    "        if s1.delegate_id == s2.delegate_id:\n",
    "            afwijkingen[s1]['merge_propose'] = (s1.begin, s2.end)\n",
    "        afwijkingen[s1]['trailer'] = (s1.end, s2.begin)\n",
    "    if o[-1].end<len(ob.item):\n",
    "        afwijkingen[o[-1]]['trailer'] = (o[-1].end, len(ob.item))\n",
    "    return afwijkingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy_search.fuzzy_phrase_searcher import FuzzyPhraseSearcher\n",
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.5,\n",
    "    \"ignorecase\": True,\n",
    "    \"ngram_size\": 3,\n",
    "    \"skip_size\": 1,\n",
    "}\n",
    "def make_searcher(config=fuzzysearch_config, nms=[]):\n",
    "    pr_searcher = FuzzyPhraseSearcher(config)\n",
    "    phrases = []\n",
    "#     for nm in nms:\n",
    "#         phrase = {'phrase':nm,\n",
    "#                   'label':'',\n",
    "#                 }\n",
    "#     basephrase = [{'phrase':\"extraordinaris Gedeputeerden uyt de provincie van\",\n",
    "#                   'label':'extraordinaris',\n",
    "#                   'variants':[]}]\n",
    "    phrases = nms\n",
    "    #pmodel = PhraseModel(model=phrases, config=config)\n",
    "    #print(phrases, phrase_model)\n",
    "    pr_searcher.index_phrases(phrases)\n",
    "    return pr_searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FuzzyKeywordGrouper(run_attendancelist.make_groslijst(searchobs)).vars2graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = searchobs[obs[3]].matched_text\n",
    "for span in enumerate(so.spans):\n",
    "    s = span[1]\n",
    "    if s.type in ['unmarked', 'delegate', 'president']:\n",
    "        searchwindow = so.item[s.begin-20:s.end+20]\n",
    "        if s.pattern != '':\n",
    "            pattern = s.pattern\n",
    "        else: \n",
    "            pattern = f\"sw: {searchwindow}\"\n",
    "        print(f\"type:{s.type}, pattern: {pattern}, name: {s.delegate_name} \\n========\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for umt in so.get_unmatched_text():\n",
    "    print(fs.find_matches(umt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.phrase_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d for d in all_marked_presidents if d=='Bout']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searchobs = runner1730.searchobs\n",
    "from IPython.display import HTML\n",
    "htmlout=[]\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\\n<h4>{T}</h4>\\n\" + rest\n",
    "    if url:\n",
    "        rest += f\"\"\"<br/><br/><a href='{url}'>link naar {T}-image</a><br/>\"\"\"\n",
    "    htmlout.append(rest)\n",
    "#out.reverse()\n",
    "HTML(\"<br><hr><br>\".join(htmlout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "t_out=[]\n",
    "for T in obs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\"\"\\n<tr><td><strong>{T}</strong></td><td>{rest}</td>\"\"\"\n",
    "#     if url:\n",
    "#         rest += f\"\"\"<td><a href='{url}'>link naar {T}-image</a></td>\"\"\"\n",
    "    rest += \"</tr>\"\n",
    "    t_out.append(rest)\n",
    "#out.reverse()\n",
    "outtable = \"\".join(t_out)\n",
    "HTML(f\"<table>{outtable}</table>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"/Users/rikhoekstra/Downloads/{year}_sample_check.html\", 'w') as flout:\n",
    "    flout.write(f\"<html><body><h1>results for {year}</h1>\\n\")\n",
    "    flout.write(f\"<table>{outtable}</table>\")\n",
    "    flout.write(\"</body></html>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample correctness results\n",
    "\n",
    "- sample of 25 days from 1730 after corrected run\n",
    "\n",
    "__results__:\n",
    "- 0,229 totaal fout\n",
    "- 0,105121294\tfout excl vz\n",
    "- 0,123989218\tfout vz\n",
    "- 0,770889488\tgoed (ex vz)\n",
    "\n",
    "__explanation__:\n",
    "- no text was recognized falsely\n",
    "- 1 day was parsed wrong, meaning there was no separation between text and attendance list, but still most of the delegates were recognized correctly\n",
    "- 2 presidents were wrong (only 'vander' recognized, instead of 'vander Waayen' or 'vander Steen'\n",
    "- no extraordinaris gedelegeerde uyt de provincie etc were wrong, but 'met vier' and 'met vijf' not recognized\n",
    "- 77 percent were complete and correctly recognized\n",
    "- in total 23 percent was not completely recognized\n",
    "- 54 % of the mistakes more  percent were recognized partially correct (some prepositions were left out)\n",
    "- 46 percent of the mistakes was not recognized, this is 11 percent of all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchobs = runner1730.searchobs\n",
    "from IPython.display import HTML\n",
    "htmlout=[]\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\\n<h4>{T}</h4>\\n\" + rest\n",
    "    if url:\n",
    "        rest += f\"\"\"<br/><br/><a href='{url}'>link naar {T}-image</a><br/>\"\"\"\n",
    "    htmlout.append(rest)\n",
    "#out.reverse()\n",
    "HTML(\"<br><hr><br>\".join(htmlout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gl = run_attendancelist.make_groslijst(searchobs)\n",
    "#make_groslijst(presentielijsten=self.searchobs)\n",
    "c = Counter(gl)\n",
    "tussenkeys = run_attendancelist.FuzzyKeywordGrouper(keyword_list=list(c.keys()))\n",
    "dralist = tussenkeys.vars2graph()\n",
    "# unmarked_text = sweep_list(dralist, junksweeper=self.junksweeper)\n",
    "# return unmarked_text\n",
    "dralist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner1730.find_unmarked_text(sweep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "This actually works very well. We need to check on the unmarked text to see what is still left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check delegates on their neighbours.\n",
    "\n",
    "As explained above, delegates do not appear randomly in in the attendance lists, as these are dictated by the order of the provinces they represent. Because the group of delegates evolves over time and may vary any given day, there is no fixed order of delegates, but still we may assume often delegates have the same neighbours in the attendance list. Therefore we calculate the likely neighbours for delegates from the raw lists of spans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a sliding window for the spans of a day\n",
    "from collections import deque\n",
    "\n",
    "def window(seq, n=5):\n",
    "    it = iter(seq)\n",
    "    win = deque((next(it, None) for _ in range(n)), maxlen=n)\n",
    "    yield win\n",
    "    append = win.append\n",
    "    for e in it:\n",
    "        append(e)\n",
    "        yield win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate most common last and first delegate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspans = Counter()\n",
    "fspans = Counter()\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    #spns = [(s.delegate_id, s.delegate_name, s.pattern) for s in ob.spans if s.type in ['delegate', 'president'] and s.pattern is not '']\n",
    "    obspans = [s for s in ob.spans if s.type in ['delegate']]\n",
    "    if len(obspans)>0:\n",
    "        lspans.update([obspans[-1].delegate_name])\n",
    "        fspans.update([obspans[0].delegate_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspans.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspans.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchobs = runner1730.searchobs\n",
    "wndws = []\n",
    "delegates = {}\n",
    "\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    #spns = [(s.delegate_id, s.delegate_name, s.pattern) for s in ob.spans if s.type in ['delegate', 'president'] and s.pattern is not '']\n",
    "    spns = [s.delegate_name for s in ob.spans if s.type in ['delegate'] and s.pattern is not '']\n",
    "    for x in window(spns):\n",
    "        delgates = x \n",
    "        d0 = delgates[0]\n",
    "        d1 = delgates[1] or (0, 'niemand')\n",
    "        d2 = delgates[2]\n",
    "        if d1 not in delegates.keys():\n",
    "            delegates[d1]={'before': Counter(),\n",
    "                           'after': Counter()}\n",
    "        delegates[d1]['before'].update([d0])\n",
    "        delegates[d1]['after'].update([d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xres = []\n",
    "for d in delegates:\n",
    "    xitem = delegates[d]\n",
    "    r = [d[1], xitem['before'].most_common(1)[0][0], xitem['after'].most_common(1)[0][0]]\n",
    "    xres.append(r)\n",
    "    #print(f\"{d}, common before: {xitem['before'].most_common(1)}, common after: {xitem['after'].most_common(1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lspans.most_common(5):\n",
    "    print(item[0], delegates.get(item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in fspans.most_common(5):\n",
    "    print(item[0], delegates.get(item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xres:\n",
    "    if x[1][0]==None:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list(list(x) for x in window(spns))[0]:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(wndws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "z = []\n",
    "matches = defaultdict(list)\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T]\n",
    "    for s in ob.matched_text.spans:\n",
    "        if s.type in ['delegate', 'president'] and s.pattern == '':\n",
    "            proposed_pattern = ob.text[s.begin:s.end]\n",
    "            spl = re.split(r'[.,-]+', proposed_pattern)\n",
    "            if len(spl)>1:\n",
    "                proposed_pattern = spl[0]\n",
    "            z.append(proposed_pattern)\n",
    "            matches[proposed_pattern] = ob.text[s.begin:s.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "z = []\n",
    "matches = defaultdict(list)\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T]\n",
    "    for s in ob.matched_text.spans:\n",
    "        if s.type in ['delegate', 'president'] and s.pattern == '':\n",
    "            proposed_pattern = ob.text[s.begin:s.end]\n",
    "            \n",
    "            z.append(proposed_pattern)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for ob in allspans:\n",
    "    res = []\n",
    "    for s in allspans[ob][\"spans\"]:\n",
    "        res[s.get('delegate_name')] = s.get(' pattern')\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy_search.fuzzy_phrase_searcher import FuzzyPhraseSearcher\n",
    "from fuzzy_search.fuzzy_phrase_model import PhraseModel\n",
    "\n",
    "# highger matching thresholds for higher quality OCR/HTR (higher precision, recall should be good anyway)\n",
    "# lower matching thresholds for lower quality OCR/HTR (higher recall, as that's the main problem)\n",
    "config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.8,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 2,\n",
    "    \"skip_size\": 2,\n",
    "}\n",
    "\n",
    "# initialize a new searcher instance with the config\n",
    "fuzzy_searcher = FuzzyPhraseSearcher(config)\n",
    "\n",
    "# create a list of domain keywords and phrases\n",
    "#variants = [{'phrase': k, 'variants': v['variants']} for k, v in runner1715.all_matched.items()]\n",
    "variants = []\n",
    "for k, v in runner1715.all_matched.items():\n",
    "    vrnts = [f[0] for f in v['variants']]\n",
    "    vrnts.append(f\"{v['variants'][0][0]}.DE\")\n",
    "    vrnt = {'phrase': v['name'],'variants':vrnts}\n",
    "    variants.append(vrnt)\n",
    "# create a PhraseModel object from the domain phrases\n",
    "phrase_model = PhraseModel(model=variants,)\n",
    "#phrase_model.add_phrases(ekwz.keys())\n",
    "# phrase_model.add_variants(variants)\n",
    "fuzzy_searcher.index_phrase_model(phrase_model=phrase_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "phrase_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = fuzzy_searcher.find_matches(text='Tamminga', include_variants=True,  use_word_boundaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i in list(set(z)):\n",
    "    result[i] = fuzzy_searcher.find_matches(i, include_variants=True, use_word_boundaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(match.variant.exact_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in r:\n",
    "    print(match.levenshtein_similarity, match.character_overlap, match.ngram_overlap, match.variant)\n",
    "    print(score_levenshtein_distance_ratio(str(match.variant.exact_string),str(match.phrase.exact_string) ), str(match.variant.exact_string),str(match.phrase.exact_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from numpy import argmax\n",
    "def score_match(match):\n",
    "    result  = sum([match.levenshtein_similarity, match.character_overlap, match.ngram_overlap, score_levenshtein_distance_ratio(str(match.variant.exact_string),str(match.phrase.exact_string))])\n",
    "    return result\n",
    "\n",
    "softscore = softmax([score_match(match) for match in r])\n",
    "i = argmax(softscore)\n",
    "r[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fuzzy_searcher.find_matches(text=\"Heer. dikke Heeren\", include_variants=True, use_word_boundaries=False, allow_overlapping_matches=True)\n",
    "for item in res:\n",
    "    print(item.score_levenshtein_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchsearch = FuzzySearcher(config=fuzzysearch_config)\n",
    "phrase\n",
    "kws = defaultdict(list)\n",
    "matcher = {}\n",
    "for key in all_matched:\n",
    "    variants = all_matched[key].get('variants')\n",
    "    keyword = all_matched[key].get('m_kw')\n",
    "    idnr = all_matched[key].get('id')\n",
    "    name = all_matched[key].get('name')\n",
    "    for variant in variants:\n",
    "        kws[keyword].append(variant[0])\n",
    "        matcher[variant[0]] = {'kw':keyword, 'id':idnr, 'name':name}\n",
    "matchsearch.index_keywords(list(kws.keys()))\n",
    "for k in kws:\n",
    "    for v in kws[k]:\n",
    "        matchsearch.index_spelling_variant(keyword=k,variant=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list(set(z)):\n",
    "    print(runner1715.matchfnd.match_candidates(text=item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match and span object takes the unmarked text from the objects and tries to match them with a delegate using the code for identifying delegates. But the failures in the ocr complicates this so that we have to use isolated strings, instead of splitting the text by commas or periods and newlines. This  drops all sorts of prefixes and in the case of a composite name such as _Taats van Amerongen_ it also either makes a name match twice or match part of it with another name. Also the prefixes like _van_ can get matched to the wrong names. So what we will do is match the names and then return to the list of spans and see if we cannot make a better match with the whole names. We would also like  to include previously acquired knowledge for better matching, but we have to think about that more.\n",
    "Another thing is that we may overfit existing names, but we'll see about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy_search.fuzzy_searcher import FuzzySearcher\n",
    "from fuzzy_search.fuzzy_phrase_model import PhraseModel\n",
    "\n",
    "config = {'char_match_threshold': 0.7,\n",
    " 'ngram_threshold': 0.6,\n",
    " 'levenshtein_threshold': 0.6,\n",
    " 'ignorecase': False,\n",
    " 'ngram_size': 2,\n",
    " 'skip_size': 2}\n",
    "\n",
    "sps = FuzzySearcher(config)\n",
    "\n",
    "spns = [(s.pattern, s.get_delegate()) for s in ob.spans if s]\n",
    "kws = []\n",
    "for s in spns:\n",
    "    nm = s[1]['name']\n",
    "    d_id = s\n",
    "#    if nm == 'Taets van Amerongen':\n",
    "\n",
    "phrase_model = PhraseModel(model=variants, )\n",
    "phrase_model.add_phrases(kws)\n",
    "sps.index_phrase_model(phrase_model=phrase_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cand = sps.find_candidates(ob.matched_text.item)\n",
    "cand = cand[0]\n",
    "span = (cand[\"match_offset\"], cand[\"match_offset\"]+len(cand['match_string']))\n",
    "ob.matched_text.set_span(clas='delegate', delegate_id=18838, delegate_name=cand['match_term'], span=span)\n",
    "ob.matched_text.get_fragments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchsearch = FuzzyPhraseSearcher(config=fuzzysearch_config)\n",
    "kws = defaultdict(list)\n",
    "matcher = {}\n",
    "for key in all_matched:\n",
    "    variants = all_matched[key].get('variants')\n",
    "    keyword = all_matched[key].get('m_kw')\n",
    "    idnr = all_matched[key].get('id')\n",
    "    name = all_matched[key].get('name')\n",
    "    for variant in variants:\n",
    "        kws[keyword].append(variant[0])\n",
    "        matcher[variant[0]] = {'kw':keyword, 'id':idnr, 'name':name}\n",
    "matchsearch.index_keywords(list(kws.keys()))\n",
    "for k in kws:\n",
    "    for v in kws[k]:\n",
    "        matchsearch.index_spelling_variant(keyword=k,variant=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deputyregister = defaultdict(list)\n",
    "search_results = {}\n",
    "fm = FndMatch(year=1728,\n",
    "                 #patterns=pats,\n",
    "                 register=register,\n",
    "                 rev_graph=transposed_graph,\n",
    "                 searcher=herensearcher,\n",
    "                 junksearcher=junksweeper,\n",
    "                 df=abbreviated_delegates)\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    mo = MatchAndSpan(ob, junksweeper=junksweeper, previously_matched=previously_matched, match_search=matchsearch)\n",
    "    for span in ob.spans:\n",
    "        if span.type in ['president', 'delegate']:\n",
    "            if span.pattern == '':\n",
    "                span.set_pattern(pattern=ob.item[span.begin:span.end])\n",
    "            r = fm.match_candidates(heer=span.pattern)\n",
    "            span.set_delegate(delegate_id=r.id, \n",
    "                              delegate_name=r.proposed_delegate, \n",
    "                              delegate_score=r.score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for keyword in herensearcher.keyword_index:\n",
    "    junk_is = junksweeper.find_candidates(text=keyword, include_variants=True)\n",
    "    if junk_is:\n",
    "        score = max(junk_is, key=lambda x: x['levenshtein_distance'])\n",
    "        if score['match_keyword']==keyword:\n",
    "            print(keyword, score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zs = [set(item.form) for item in vrts]\n",
    "c = {s[0]:{'default':s[1],'variants':defaultdict(int)} for s in enumerate(n)}\n",
    "for s in enumerate(zip(n, vrts[0].form)):\n",
    "    compared = s[1]\n",
    "    variants = c[s[0]]['variants']\n",
    "    if compared[0]!=compared[1]:\n",
    "        variants[compared[1]]+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "moregentlemen[7].variants"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = {}\n",
    "for heer in moregentlemen:\n",
    "    vrts = heer.variants['general']\n",
    "    results[heer.name] = [nw(heer.name,variant.form, mismatch=2, gap=0) for variant in vrts]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lc = (0,'')\n",
    "for item in results['van Isselmuden tot Zwollingerkamp']:\n",
    "    i = item[1]\n",
    "    l = len([c for c in i if c not in['-','.']])\n",
    "    if l > lc[0]:\n",
    "        lc = (l, i)\n",
    "lc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def nw(x, y, match = 1, mismatch = 1, gap = 1):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    # Optimal score at each possible pair of characters.\n",
    "    F = np.zeros((nx + 1, ny + 1))\n",
    "    F[:,0] = np.linspace(0, -nx, nx + 1)\n",
    "    F[0,:] = np.linspace(0, -ny, ny + 1)\n",
    "    # Pointers to trace through an optimal aligment.\n",
    "    P = np.zeros((nx + 1, ny + 1))\n",
    "    P[:,0] = 3\n",
    "    P[0,:] = 4\n",
    "    # Temporary scores.\n",
    "    t = np.zeros(3)\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            if x[i] == y[j]:\n",
    "                t[0] = F[i,j] + match\n",
    "            else:\n",
    "                t[0] = F[i,j] - mismatch\n",
    "            t[1] = F[i,j+1] - gap\n",
    "            t[2] = F[i+1,j] - gap\n",
    "            tmax = np.max(t)\n",
    "            F[i+1,j+1] = tmax\n",
    "            if t[0] == tmax:\n",
    "                P[i+1,j+1] += 2\n",
    "            if t[1] == tmax:\n",
    "                P[i+1,j+1] += 3\n",
    "            if t[2] == tmax:\n",
    "                P[i+1,j+1] += 4\n",
    "    # Trace through an optimal alignment.\n",
    "    i = nx\n",
    "    j = ny\n",
    "    rx = []\n",
    "    ry = []\n",
    "    while i > 0 or j > 0:\n",
    "        if P[i,j] in [2, 5, 6, 9]:\n",
    "            rx.append(x[i-1])\n",
    "            ry.append(y[j-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif P[i,j] in [3, 5, 7, 9]:\n",
    "            rx.append(x[i-1])\n",
    "            ry.append('-')\n",
    "            i -= 1\n",
    "        elif P[i,j] in [4, 6, 7, 9]:\n",
    "            rx.append('-')\n",
    "            ry.append(y[j-1])\n",
    "            j -= 1\n",
    "#     if ry[-1] != rx[-1]:\n",
    "#         ry[-1] =  '-'\n",
    "    # Reverse the strings.\n",
    "    rx = ''.join(rx)[::-1]\n",
    "    ry = ''.join(ry)[::-1]\n",
    "\n",
    "    return [rx, ry]\n",
    "    #return '\\n'.join([rx, ry])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from itertools import starmap\n",
    "n = 'Van Singendonk'\n",
    "setn = set(n)\n",
    "sets = [set(item.form) for item in vrts]\n",
    "list(starmap(setn.intersection, sets))\n",
    "vrts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchfnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Continue matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defo = defaultdict(list)\n",
    "for item in ob.get_unmatched_text():\n",
    "    item = [i.strip() for i in re.split('[\\.|,]', item)]\n",
    "    for i in item:\n",
    "        identified = ''\n",
    "        res = {}\n",
    "        r2 = {}\n",
    "        for g in moregentlemen:\n",
    "            mn = 0\n",
    "            co = 0\n",
    "            mc = 0\n",
    "            vrts = g.variants['general']\n",
    "            dr = [score_levenshtein_distance_ratio(x.form, i) for x in vrts if x.heerid==g.heerid]\n",
    "            if len(dr)>0:\n",
    "                mn = mean(dr)\n",
    "            res[g] = {'mn':mn}\n",
    "            if len(i)<5:\n",
    "                co = [score_ngram_overlap_ratio(x.form, i, 1) for x in vrts if x.heerid==g.heerid]\n",
    "                if co and len(co)>0:\n",
    "                  mc = mean(co)\n",
    "            res[g]['mc'] = mc\n",
    "        best_l = max(res, key=lambda key: res[key]['mn'])\n",
    "        best_c = max(res, key=lambda key: res[key]['mc'])\n",
    "        if res[best_l]['mn'] > 0.5:\n",
    "            identified = res[best_l]['mn']\n",
    "            if best_c != 0:\n",
    "                if res[best_c] != identified:\n",
    "                    identified = res[best_c]\n",
    "        if identified != '':\n",
    "            delegate = fm.match_candidates(heer=best_l.name)\n",
    "            identified = delegate\n",
    "        else: \n",
    "            identified = None\n",
    "        if identified:\n",
    "            \n",
    "            name = delegate.proposed_delegate\n",
    "            m_id = delegate.id\n",
    "        else:\n",
    "            name = ''\n",
    "            m_id = 0\n",
    "        start = ob.item.find(i)\n",
    "        end = start + len(i)\n",
    "        ob.set_span(span=(start, end),pattern=i, clas='deputy', delegate_name=name,\n",
    "                    delegate_id=m_id)\n",
    "#         defo[i]={'mn':(best_l,res[best_l]['mn']),\n",
    "#                  'co':(best_c,res[best_c]['mc'])}\n",
    "# defo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mn(k, defo=defo):\n",
    "    defo = dict(defo)\n",
    "    v = defo[k]['mn'][1]\n",
    "    v = float(v)\n",
    "    return v\n",
    "    \n",
    "#         mn = dict(defo[k].get('mn').value\n",
    "for k in defo:\n",
    "    v = get_mn(k)\n",
    "    if v < 0.5:\n",
    "        res = iterative_search(name=k, year=1728, df=abbreviated_delegates)\n",
    "        res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = moregentlemen[8]\n",
    "print(s.variants)\n",
    "for v in s.variants:\n",
    "    print(score_levenshtein_distance_ratio('an Schwartzenberg', s.name))\n",
    "    #list(map(lambda x: score_levenshtein_distance_ratio('an Schwartzenberg', x), s.variants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = s.variants['general'][0]\n",
    "v.form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fr = ob.matched_text.get_fragments()\n",
    "\n",
    "if prezidents:\n",
    "    prez = prezidents[0]\n",
    "    begin = prez['begin']\n",
    "else:\n",
    "    begin = 0\n",
    "for fragment in fr:\n",
    "    if fragment['type'] == 'unmarked' and fragment['end'] > begin:\n",
    "        f = fragment['pattern']\n",
    "        f == ''\n",
    "        f = ob.matched_text.item[fragment['begin']:fragment[end]]\n",
    "        fragments.append(f)\n",
    "fragments\n",
    "#ob.matched_text.get_unmatched_text()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for span in ob.spans:\n",
    "        if span.type in ['president', 'delegate']:\n",
    "            if span.pattern == '':\n",
    "                span.set_pattern(pattern=ob.item[span.begin:span.end])\n",
    "            r = fm.match_candidates(heer=span.pattern)\n",
    "            span.set_delegate(delegate_id=r.id, \n",
    "                              delegate_name=r.proposed_delegate, \n",
    "                              delegate_score=r.score)\n",
    "print(ob.get_fragments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span.pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span.set_delegate(delegate_id=10, \n",
    "#                               delegate_name='blop', \n",
    "#                               delegate_score=1.0)\n",
    "span.delegate_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matched.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Evaluations (fwiw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(previously_matched.aggregate(\"mean\")[['levenshtein_distance', 'score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_matched.score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_matched.levenshtein_distance.round(2).value_counts().sort_values('index', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in ob.spans:\n",
    "    print(s, s.idnr, ob.item[s.begin:s.end], s.get_delegate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a better searcher and identifier.\n",
    "\n",
    "The problem: \n",
    "We have two ways of identifying name patterns now.\n",
    "\n",
    "- the first uses pandas based method to identify strings against a database of known delegates and then pulls in some of the external context. This works pretty well for initial identification, but not all names are recognized very well. There is especially a problem by names that have been badly garbled by the text recognition, which is now the case with the OCR, but it will probably not get better if we start using HTR and look up entities in the text. Another issue is that this method has no memory of earlier work, both in the sense that it will not know if earlier matches were found or earlier proposals were marked as false. \n",
    "- the fuzzysearcher is very good for spotting name patterns in text as it includes approximate text matching and a number of variants. But it knows nothing about identification and there is no easy way of associating patterns with entities.\n",
    "- fuzzysearcher is also able to cluster alike word patterns and recluster heterogeneous lists. However, this does not per se bring us closer to identification\n",
    "- the only way to include previously selected variants is in the fuzzy searcher. Which is useful, but the main problem remains associating this with external knowledge.\n",
    "\n",
    "This leads to the following requirements:\n",
    "- a way of associating patterns with identities. But how. Earlier I put the result of a Fuzzysearch pattern clustering in a dataframe, but this included heterogeneous patterns. One possibility is to revise candidates and recategorize weak candidates. The goals is to incrementally improve the patterns associated with a name. This is especially useful if we reuse the objects over many years.\n",
    "- another idea is to check for internal consistency of the results and try and find inconsistent results. This is especially useful for the presidents, as their number is even more limited than the delegates. The problem is overfitting in which unidentified names get associated with identified names and also new or infrequent delegates may drop out. SO there is a need for manual check.\n",
    "- another tool may be to make a specialized confusion matrix for delegates\n",
    "\n",
    "We start with this latest\n",
    "no we don't. Most of this is already in place. The main problem is that we cannot access the it from 'both' sides, so that we have to rematch all over again. Which is what we do not want, because it slows down the process *and* it introduces mistakes all over again. What we need is memory:\n",
    "- of previous matches\n",
    "- of previous associations\n",
    "- of previous wrong associations\n",
    "and perhaps more\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delegates_from_spans(ob):\n",
    "    result = {}\n",
    "    for s in ob.spans:\n",
    "        if s.type in ['delegate', 'president']:\n",
    "            result['delegate'] = s.get_delegate()['name']\n",
    "            result['pattern'] = s.pattern\n",
    "            result['type'] = s.type\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_delegates = [get_delegates_from_spans(searchobs[ob].matched_text) for ob in searchobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "reduced_delegates = defaultdict(Counter)\n",
    "for item in all_delegates:\n",
    "    reduced_delegates[item.get('delegate')].update([item.get('pattern')])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moregentlemen = runner1730.moregentlemen\n",
    "from statistics import mean\n",
    "from fuzzy_search.fuzzy_string import score_ngram_overlap_ratio\n",
    "from run_attendancelist import iterative_search, abbreviated_delegates, found_delegates, MatchHeer\n",
    "day = year = 1730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = []\n",
    "nwheren = []\n",
    "clean_gentlemen = []\n",
    "for g in moregentlemen:\n",
    "    matches = defaultdict(list)\n",
    "    matchkw = g.matchkw\n",
    "    matchkw = matchkw.strip()\n",
    "    nwmatches = {}\n",
    "    if matchkw == '':\n",
    "        unclassified = g.variants['general'] # we go to the next instance and look into this later\n",
    "        continue\n",
    "    for v in g.variants['general']:\n",
    "        matches[v.match].append(v)\n",
    "        for key in matches.keys():\n",
    "            name = g.name\n",
    "            nwmatches[matchkw] = []\n",
    "            s = score_levenshtein_distance_ratio(key, matchkw)\n",
    "            if s > 0.7:\n",
    "                nwmatches[matchkw].extend(matches[key])\n",
    "            else:\n",
    "                if not re.search('\\w+', key):\n",
    "                    #this is an empty key\n",
    "                    meanscore = mean([score_ngram_overlap_ratio(kw.form, matchkw, 1) for kw in matches[key]])\n",
    "                    if meanscore > 0.8:\n",
    "                         nwmatches[matchkw].extend(matches[key])\n",
    "        #            print('meanscore: ', key, meanscore)\n",
    "\n",
    "                # is there no strange 'Van' variant in this?\n",
    "                elif len(key)<7:\n",
    "                    s2 = max([score_levenshtein_distance_ratio(key.lower(), van) for van in ['van','vander']])\n",
    "                    if s2 > 0.8:\n",
    "                        meanscore = mean([score_levenshtein_distance_ratio(kw.form, matchkw) for kw in matches[key]])\n",
    "                        nwmatches[matchkw].extend(matches[key])\n",
    "                        #print('meanscore: ', key, meanscore)\n",
    "                    else: \n",
    "                        unclassified.extend(matches[key])\n",
    "                else:\n",
    "                    meanheerscore = mean([score_levenshtein_distance_ratio(kw.form, matchkw) for kw in matches[key]])\n",
    "                    meankeyscore = mean([score_levenshtein_distance_ratio(kw.form, key) for kw in matches[key]])\n",
    "                    meanheergramscore = mean([score_ngram_overlap_ratio(kw.form, matchkw, 2) for kw in matches[key]])\n",
    "                    meankeygramscore = mean([score_ngram_overlap_ratio(kw.form, key, 2) for kw in matches[key]])\n",
    "                    if meankeyscore>meanheerscore and meankeygramscore>meanheergramscore:\n",
    "                        #print(key, meankeyscore-meanheerscore, meankeygramscore-meanheergramscore)\n",
    "                        nwres = iterative_search(key, year=day, df=abbreviated_delegates)\n",
    "                        rec = {'id': nwres.id.iat[0],\n",
    "                               'm_kw': key,\n",
    "                               'name': nwres.name.iat[0],\n",
    "                               'variants': [(v.form, v.match, v.score) for v in matches[key]]}\n",
    "                        nwheer = MatchHeer(rec=rec)\n",
    "                        nwheren.append(nwheer)\n",
    "    g.variants['new']=nwmatches\n",
    "        \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in moregentlemen:\n",
    "    vrs = i.variants['general']\n",
    "    n = i.name\n",
    "    m = i.matchkw\n",
    "    print(n, list(filter(lambda x: score_levenshtein_distance_ratio(x,n)>0.7, vrs)))\n",
    "    print(m, list(filter(lambda x: score_levenshtein_distance_ratio(x,m)>0.7, vrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "allvariants = Counter()\n",
    "for g in runner1730.moregentlemen:\n",
    "    allvariants.update(g.variants['general'])\n",
    "allvariants.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in runner1730.moregentlemen:\n",
    "    print(i, i.variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so we need something do discriminated them. Perhaps the time of their actual delegate appointments?\n",
    "another idea is to try and match names more closely. But we do not implement that here (as I do not think there is much to gain here as yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "hs = iterative_search('Heekeren', year=1728,df=abbreviated_delegates)\n",
    "persoonids = hs.id.unique()\n",
    "list(persoonids)\n",
    "sids = [f\"{persoonid}\" for persoonid in persoonids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Checks\n",
    "## Not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmt = [searchobs[so].matched_text.get_unmatched_text() for so in searchobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrds = [k[0].form for k in allvariants.most_common() if k[0] not in ['',None]]\n",
    "fks.find_close_distance_keywords(wrds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nwh in nwheren:\n",
    "    if nwh.heerid not in all_matched.keys():\n",
    "        print(nwh.heerid, nwh.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinheer(h1, h2):\n",
    "    if not h1.heerid == h2.heerid:\n",
    "        raise ValueError('ids not equal')\n",
    "    forms1 = set([v.form for form in h1.variants])\n",
    "    forms2 = set([v.form for form in h2.variants])\n",
    "    difference = forms2.difference(forms1)\n",
    "    \n",
    "    for form in difference:\n",
    "        v1 = [v for v in h2.variants['general'] if v.form==form][0]\n",
    "        h1.variants['general'].append(v1)\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnwheren = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idvals = set([n.heerid for n in nwheren])\n",
    "clusterednwh = defaultdict(list)\n",
    "for heerid in idvals:\n",
    "    clusterednwh[heerid] = [nwh for nwh in nwheren if nwh.heerid == heerid]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "ds = {}\n",
    "sh = sorted(nwheren, key=lambda x: x.heerid)\n",
    "for k, g in groupby(sh, key=lambda x: x.heerid):\n",
    "    ds[k] = list(g)\n",
    "for k in ds:\n",
    "    h1 = ds[k][0]\n",
    "    for h2 in ds[k][1:]:\n",
    "        h1 = joinheer(h1,h2)\n",
    "    nnwheren.append(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnwheren[2].variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unob = unclassified[0]\n",
    "unob.heer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(unclassified)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclas = fks.find_close_distance_keywords([v.form for v in unclassified])\n",
    "G_unclas = nx.Graph()\n",
    "d_nodes = sorted(unclas)\n",
    "for node in d_nodes:\n",
    "    attached_nodes = unclas[node]\n",
    "    G_unclas.add_node(node)\n",
    "    for nod in attached_nodes:\n",
    "        G_unclas.add_edge(node, nod)\n",
    "connected_unclas = list(nx.connected_components(G_unclas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_unclas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_search('Ten Brincke', year=1728, df=abbreviated_delegates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_search('Eck', year=1728, df=abbreviated_delegates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in connected_unclas:\n",
    "    for i in x:\n",
    "        if not re.search('[0-9]+',i):\n",
    "            print (i)\n",
    "#         ir = [v for v in unclassified if v.form == i]\n",
    "#         for vir in ir:\n",
    "#             print (vir.form, vir.heerid, '\\n\\n')\n",
    "            result = herensearcher.find_candidates(i)\n",
    "            if len(result) > 0:\n",
    "                eindresult = max(result, key=lambda keyword: keyword['levenshtein_distance'])\n",
    "                "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[g for g in moregentlemen if score_ngram_overlap_ratio(g.matchkw, 'Taats van Amerongen', 1) > 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "iterative_search('Goslinga', year=day, df=abbreviated_delegates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_checker(ob):\n",
    "    \"\"\"prints all spans from a searchobject\"\"\"\n",
    "    spans = ob.matched_text.spans\n",
    "    for span in spans:\n",
    "        txt = ob.matched_text.item[span.begin:span.end]\n",
    "        span.set_pattern(txt)\n",
    "\n",
    "        #kand = matchsearch.find_candidates(txt)[0]\n",
    "        msk = framed_gtlm.vs.apply(lambda x: levenst_vals(x, txt))\n",
    "        mres = framed_gtlm.loc[msk==msk.max()]\n",
    "        if len(mres)>0:\n",
    "\n",
    "    #         setattr(span, 'delegate_id', mres.uuid.iat[0])\n",
    "    #         setattr(span, 'delegate_name', mres.name.iat[0])\n",
    "        #print(kand, all_matched[kand])\n",
    "            span.set_delegate(delegate_id=mres.ref_id.iat[0], delegate_name=mres.name.iat[0], delegate_score=msk.max())\n",
    "        print(span.type, span.idnr, txt, span.delegate_id, span.delegate_name, span.delegate_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from republic.helper.similarity_match import score_levenshtein_distance\n",
    "twijfelresults = []\n",
    "for im in runner1730.all_matched:\n",
    "    item = runner1730.all_matched[im]\n",
    "    if score_levenshtein_distance(item['m_kw'], item['name'])<0.6:\n",
    "        twijfelresults.append(im)\n",
    "twijfelresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(runner1730.all_matched) - len(twijfelresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner1730.all_matched[17083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match2pattern(match:dict, bestpattern='', threshold=0.9, year=0):\n",
    "    goodresults = []\n",
    "    quarantine = []\n",
    "    variants = match['variants']\n",
    "    if bestpattern == '':\n",
    "            try:\n",
    "                bestpattern = max(variants, key=lambda x: x[2])[0]\n",
    "            except TypeError:\n",
    "                print (variant)\n",
    "                raise\n",
    "    if match['score']>threshold:\n",
    "        results = goodresults\n",
    "    else:\n",
    "        results = quarantine\n",
    "    for variant in variants:\n",
    "        if variant[2]>threshold:\n",
    "            result = {}\n",
    "            result['variant_pattern'] = variant[0]\n",
    "            result['variant_score'] = variant[2]\n",
    "            result['variant_keyword'] = bestpattern\n",
    "            result['match_keyword']=match['m_kw']\n",
    "            result['match_name'] = match['name']\n",
    "            result['match_score'] = match['score']\n",
    "            result['match_id'] = match['id']\n",
    "            results.append(result)\n",
    "    return goodresults, quarantine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1730\n",
    "indexkws = []\n",
    "qindexkws = []\n",
    "for item in runner1730.all_matched:\n",
    "    matches = runner1730.all_matched[item]\n",
    "    i_kws, q_kws = match2pattern(matches, year=year)\n",
    "    indexkws.extend(i_kws)\n",
    "    qindexkws.extend(q_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "goodvariants = {}\n",
    "joined_good_variants = defaultdict(list)\n",
    "unique_ids = list(set([i.get('match_id') for i in indexkws]))\n",
    "\n",
    "for k in unique_ids:\n",
    "    dd = defaultdict(list)\n",
    "    ikws = [d for d in indexkws if d.get('match_id') == k]\n",
    "    for d in ikws: # you can list as many input dicts as you want here\n",
    "        for key, value in d.items():\n",
    "            if key != 'match_id':\n",
    "                dd[key].append(value)\n",
    "    for kie, v in dd.items():\n",
    "        v = list(set(v))\n",
    "        if len(v) == 1:\n",
    "            v = v[0]\n",
    "        dd[kie] = v\n",
    "    goodvariants[k] = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "goodvariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proposed_good_matches = []\n",
    "for item in qindexkws:\n",
    "    if item['variant_score'] > item['match_score']:\n",
    "        item['match_keyword'] = item['variant_pattern']\n",
    "        item['match_score'] = item['variant_score']\n",
    "        proposed_good_matches.append(item)\n",
    "        qindexkws.pop(qindexkws.index(item)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs = [i['variant_pattern'] for i in indexkws]\n",
    "vids = [i['match_id'] for i in indexkws]\n",
    "prids = [i['match_id'] for i in proposed_good_matches]\n",
    "vids.extend(prids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "proposed_good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in qindexkws:\n",
    "    if item['match_id'] in vids:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qindexkws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proposed_good_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "mean([i.get('match_score') for i in indexkws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([i['match_score'] for i in indexkws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from republic.analyser.attendance_lists.identify import iterative_search\n",
    "reslt = {}\n",
    "for i in twijfelresults:\n",
    "    item = runner1730.all_matched[i]\n",
    "    l = 0\n",
    "    v = ''\n",
    "    v = max(item['variants'], key=lambda z: len(z[0]))[0]\n",
    "    reslt[v] = iterative_search(name=v, year=1730, debug=False, df=runner1730.abbreviated_delegates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "for i in reslt:\n",
    "    display(i, reslt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G_heren = nx.Graph()\n",
    "d_nodes = sorted(cl_heren)\n",
    "for node in d_nodes:\n",
    "    attached_nodes = cl_heren[node]\n",
    "    G_heren.add_node(node)\n",
    "    for nod in attached_nodes:\n",
    "        G_heren.add_edge(node, nod)\n",
    "pm_heren = list(nx.connected_components(G_heren))\n",
    "pm_heren"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out = []\n",
    "for T in ra.searchobs:\n",
    "    ob = ra.searchobs[T]\n",
    "    out.append(ob.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from republic.fuzzy.fuzzy_keyword_searcher import FuzzyPhraseSearcher\n",
    "fuzzysearch_config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.5,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 2,\n",
    "    \"skip_size\": 2,\n",
    "}\n",
    "fks = FuzzyPhraseSearcher(config=fuzzysearch_config)\n",
    "fks.use_word_boundaries = False\n",
    "kws = [goodvariants[p]['match_keyword'] for p in goodvariants]\n",
    "false_kws = ['PRAESIDE','PRAESENTIBUS', 'Nihil Actum']\n",
    "kws.extend(false_kws)\n",
    "fks.index_keywords(kws)\n",
    "for kw in kws:\n",
    "    if kw not in false_kws:\n",
    "        z = [goodvariants[d] for d in goodvariants if goodvariants[d]['match_keyword'] == kw][0]\n",
    "        vrs = z[\"variant_pattern\"]\n",
    "        for v in vrs:\n",
    "            fks.index_spelling_variant(kw, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspect_obs = defaultdict(list)\n",
    "for ob in runner1730.searchobs:\n",
    "    fragments = runner1730.searchobs[ob].get_fragments()\n",
    "    fragments.reverse()\n",
    "    for item in fragments:\n",
    "        lastdelegate = ''\n",
    "        ld = item.get('delegate_name')\n",
    "        if ld in common_last_delegates:\n",
    "            break\n",
    "        elif ld == '': \n",
    "            p = item.get('pattern')\n",
    "            r = fks.find_candidates(text=p)\n",
    "            if len(r)>0:\n",
    "                r = max(r, key=lambda x: x['levenshtein_distance'])\n",
    "                if r[\"match_keyword\"] in false_kws:\n",
    "                    suspect_obs[r[\"match_keyword\"]].append(ob)\n",
    "                elif r[\"match_keyword\"] in commonlastgoodvariantsvs:\n",
    "                    begin = item['begin']\n",
    "                    changeob = runner.searchobs[ob]\n",
    "                    begin = res[-2]\n",
    "                    for span in changeob.matched_text.spans:\n",
    "                        if span.begin>=begin:\n",
    "                            changeob.matched_text.spans.remove(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "commonlastgoodvariants = {v:goodvariants[v] for v in goodvariants if goodvariants[v]['match_name'] in common_last_delegates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonlastgoodvariantsvs = []\n",
    "for v in goodvariants:\n",
    "    if goodvariants[v]['match_name'] in common_last_delegates:\n",
    "        commonlastgoodvariantsvs.extend(goodvariants[v]['variant_pattern'])\n",
    "commonlastgoodvariantsvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastspans = [f[-1].to_json() for f in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastfragments[-100]a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spans = []\n",
    "for T in ra.searchobs:\n",
    "    ob = ra.searchobs[T]\n",
    "    spans.append(ob.get_spans())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprez = {}\n",
    "for T in runner1715.searchobs:\n",
    "    ob = runner1715.searchobs[T]\n",
    "    p = [s for s in ob.get_spans() if s.type=='president']\n",
    "    if p:\n",
    "        noprez[T] = p\n",
    "    else:\n",
    "        noprez[T] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upr = list(set(runner1715.presidents))\n",
    "upr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(noprez.keys())\n",
    "ks.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprez[t][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fks.keyword_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"'Van Lynden, van Singendonck, IS ter Vergaderinge gelesen de Requeste van Johan Coenraadts Hendricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_levenshtein_distance_ratio('Van Schwartzenbergh', 'Van Schwart zenbergh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fks = make_herensearcher(keywords={i.name:[v.form for v in i.variants['general']] for i in ra.moregentlemen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_herensearcher(keywords={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ks:\n",
    "    if not noprez[t]:\n",
    "        likely_prez = get_vicinity_presidents(noprez, t, )\n",
    "        txt = ra.searchobs[t].text\n",
    "        if len(txt) > 10:\n",
    "            e = txt.find('PRAESENTIBUS')\n",
    "            if e > -1:\n",
    "                stxt = txt[:e]\n",
    "            else: \n",
    "                stxt = txt[:100]\n",
    "            print(likely_prez.find_candidates(stxt), stxt)\n",
    "                \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_attendancelist.get_delegates_from_spans(searchobs[T].matched_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ra.searchobs:\n",
    "    so = ra.searchobs[k]\n",
    "    sspans = so.get_spans()\n",
    "    for s in sspans:\n",
    "        if s.pattern in s.delegate_name:\n",
    "            if s.delegate_name > s.pattern\n",
    "            so.get_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.all_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchobs = runner1715.searchobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "htmlout=[]\n",
    "for T in searchobs:\n",
    "    ob = searchobs[T].matched_text\n",
    "    url = searchobs[T].make_url()\n",
    "    ob.mapcolors()\n",
    "    rest = ob.serialize()\n",
    "    rest = f\"\\n<h4>{T}</h4>\\n\" + rest\n",
    "    if url:\n",
    "        rest += f\"\"\"<br/><br/><a href='{url}'>link naar {T}-image</a><br/>\"\"\"\n",
    "    htmlout.append(rest)\n",
    "#out.reverse()\n",
    "HTML(\"<br><hr><br>\".join(htmlout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for T in searchobs:\n",
    "            print(searchobs[T].resumptionstart, searchobs[T].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "republic",
   "language": "python",
   "name": "republic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
