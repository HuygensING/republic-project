{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Preprocessing Index Pages\n",
    "\n",
    "- filter tiny and huge text elements (i.e. deviating from average character/word width and height\n",
    "- extract page lines that are part of the main text body containing index entries\n",
    "- insert and clean up repetition symbols in index entries\n",
    "    - determine length of repetition symbol\n",
    "    - identify and replace mis-recognized repetition symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inventory_num': 3767, 'base_dir': '../../../Data/Projects/REPUBLIC/hocr/', 'page_index': 'republic_hocr_pages', 'page_doc_type': 'page', 'scan_index': 'republic_hocr_scans', 'scan_doc_type': 'scan', 'tiny_word_width': 15, 'avg_char_width': 20, 'remove_tiny_words': True, 'remove_line_numbers': False, 'normal_scan_width': 4840, 'data_dir': '../../../Data/Projects/REPUBLIC/hocr/3767/'}\n",
      "Number of scan files: 774\n"
     ]
    }
   ],
   "source": [
    "from parse_hocr_files import make_hocr_page\n",
    "import republic_column_parser as column_parser\n",
    "from elasticsearch import Elasticsearch\n",
    "import republic_page_parser as page_parser\n",
    "import republic_paragraph_parser as paragraph_parser\n",
    "import republic_file_parser as file_parser\n",
    "\n",
    "import copy\n",
    "\n",
    "year = 1725\n",
    "inventory_num = 3767\n",
    "base_config = {\n",
    "    \"inventory_num\": inventory_num,\n",
    "    \"base_dir\": \"../../../Data/Projects/REPUBLIC/hocr/\",\n",
    "    \"page_index\": \"republic_hocr_pages\",\n",
    "    \"page_doc_type\": \"page\",\n",
    "    \"scan_index\": \"republic_hocr_scans\",\n",
    "    \"scan_doc_type\": \"scan\",\n",
    "    \"tiny_word_width\": 15, # pixel width\n",
    "    \"avg_char_width\": 20,\n",
    "    \"remove_tiny_words\": True,\n",
    "    \"remove_line_numbers\": False,\n",
    "    \"normal_scan_width\": 4840\n",
    "}\n",
    "\n",
    "\n",
    "def set_config_inventory_num(base_config, inventory_num):\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"inventory_num\"] = inventory_num\n",
    "    config[\"data_dir\"] = config[\"base_dir\"] + \"{}/\".format(inventory_num)\n",
    "    return config\n",
    "\n",
    "inventory_config = set_config_inventory_num(base_config, inventory_num)\n",
    "print(inventory_config)\n",
    "scan_files = file_parser.get_files(inventory_config[\"data_dir\"])\n",
    "print(\"Number of scan files:\", len(scan_files))\n",
    "\n",
    "scan_file = scan_files[10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from republic_index_page_parser import index_lemmata\n",
    "from collections import defaultdict\n",
    "import republic_index_page_parser as index_parser\n",
    "import republic_elasticsearch as rep_es\n",
    "\n",
    "avg_left = 0\n",
    "lemma_index = defaultdict(list)\n",
    "curr_lemma = None\n",
    "    \n",
    "\n",
    "for page_id in pages_info:\n",
    "    page_doc = rep_es.retrieve_page_doc(page_id, year_config)\n",
    "    print(\"\\n\\n\", page_id)\n",
    "    if page_doc[\"page_type\"] != \"index_page\":\n",
    "        print(\"skipping non-index page\")\n",
    "        continue\n",
    "    page_doc[\"num_page_ref_lines\"] = index_parser.count_page_ref_lines(page_doc)\n",
    "    for column_info in page_doc[\"columns\"]:\n",
    "        print(\"\\n\\n\", column_info[\"column_id\"])\n",
    "        column_hocr = column_info[\"column_hocr\"]\n",
    "        lines = index_parser.get_index_entry_lines(column_hocr)\n",
    "        curr_lemma = index_parser.index_lemmata(column_info[\"column_id\"], lines, lemma_index, curr_lemma)\n",
    "        print(\"returned lemma:\", curr_lemma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Searching of Keywords in the Resolutions\n",
    "\n",
    "Knowing which keywords should appear in the text, possibly with some spelling variation and OCR errors, we can use a fuzzy search algorithm to find candidate matches. \n",
    "\n",
    "Keywords that are similar to each other are registered as distractor terms, so matches are assigned as candidates to the nearest of sets of similar keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parse_republic_hocr_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff7d51402aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mparse_republic_hocr_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge_text_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_hocr_scan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlemma_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'parse_republic_hocr_files'"
     ]
    }
   ],
   "source": [
    "from parse_republic_hocr_files import merge_text_lines, read_hocr_scan\n",
    "\n",
    "lemma_matches = defaultdict(list)\n",
    "\n",
    "def add_context(match, page_text):\n",
    "    context = fuzzy_searcher.get_term_context(page_text, match, context_size=40)\n",
    "    match[\"match_term_in_context\"] = context[\"match_term_in_context\"]\n",
    "    match[\"context_start_offset\"] = context[\"start_offset\"]\n",
    "    match[\"context_end_offset\"] = context[\"end_offset\"]\n",
    "\n",
    "for scan_file in scan_files:\n",
    "    resolution_page_num = scan_file[\"scan_page_num\"] - 90\n",
    "    if scan_file[\"scan_page_num\"] <= 90:\n",
    "        continue\n",
    "    print(scan_file[\"scan_page_num\"], resolution_page_num)\n",
    "    hocr_page = read_hocr_scan(scan_file)\n",
    "    page_text = merge_text_lines(hocr_page)\n",
    "    matches = fuzzy_searcher.find_candidates(page_text)\n",
    "    for match in matches:\n",
    "        lemma_matches[match[\"match_keyword\"]] += [match]\n",
    "        add_context(match, page_text)\n",
    "        match[\"page_num\"] = scan_file[\"scan_page_num\"]\n",
    "        print(match[\"match_keyword\"], \"\\t\", match)\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6960c3442893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\tAantal kandidaten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemma_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tKandidaat:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"match_string\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tPagina:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemma_matches' is not defined"
     ]
    }
   ],
   "source": [
    "for lemma in sorted(lemma_matches):\n",
    "    print(\"\\n\", lemma, \"\\tAantal kandidaten:\", len(lemma_matches[lemma]), \"\\n\")\n",
    "    for match in lemma_matches[lemma]:\n",
    "        print(\"\\tKandidaat:\", match[\"match_string\"])\n",
    "        print(\"\\tPagina:\", match[\"page_num\"])\n",
    "        print(\"\\tContext:\", match[\"match_term_in_context\"][5:-5])\n",
    "        print()\n",
    "\n",
    "\n",
    "for lemma in lemma_index:\n",
    "    print(\"\\nTrefwoord:\", lemma)\n",
    "    #print(lemma_index[lemma])\n",
    "    for entry in lemma_index[lemma]:\n",
    "        pages = \", \".join([str(page_ref) for page_ref in entry[\"page_refs\"]])\n",
    "        description = entry[\"description\"][:70]\n",
    "        print(\"\\tPagina:\", pages, \"\\tBeschrijving:\", description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan 45 uneven is first resolution page\n",
    "# page num: 91\n",
    "\n",
    "from fuzzy_context_searcher import FuzzyContextSearcher\n",
    "import pandas as pd\n",
    "\n",
    "config = {\n",
    "    \"char_match_threshold\": 0.8,\n",
    "    \"ngram_threshold\": 0.6,\n",
    "    \"levenshtein_threshold\": 0.8,\n",
    "    \"ignorecase\": False,\n",
    "    \"ngram_size\": 3,\n",
    "    \"skip_size\": 0,\n",
    "}\n",
    "\n",
    "fuzzy_searcher = FuzzyContextSearcher(config)\n",
    "\n",
    "keywords = [\n",
    "    \"Admiraliteyt tot Amfterdam\", \n",
    "    \"Admiraliteyt in het Noorder Quartier\", \n",
    "    \"Admiraliteyt in Vrieslandt\", \n",
    "    \"Admiralteyt in Zeelandt\",\n",
    "    \"Varckens\"\n",
    "]\n",
    "\n",
    "distractor_terms = {\n",
    "    \"Admiraliteyt tot Amfterdam\": {\n",
    "        \"Admiraliteyt in het Noorder Quartier\", \"Admiraliteyt in Vrieslandt\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiraliteyt in het Noorder Quartier\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in Vrieslandt\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiraliteyt in Vrieslandt\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in het Noorder Quartier\", \"Admiralteyt in Zeelandt\"\n",
    "    },\n",
    "    \"Admiralteyt in Zeelandt\": {\n",
    "        \"Admiraliteyt tot Amfterdam\", \"Admiraliteyt in het Noorder Quartier\", \"Admiraliteyt in Vrieslandt\"\n",
    "    },\n",
    "}\n",
    "fuzzy_searcher.index_keywords(keywords)\n",
    "fuzzy_searcher.index_distractor_terms(distractor_terms)\n",
    "\n",
    "hocr_resolution_pages = []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
