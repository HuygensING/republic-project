{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_hocr_files import make_hocr_page\n",
    "import republic_column_parser as column_parser\n",
    "from elasticsearch import Elasticsearch\n",
    "import republic_page_parser as page_parser\n",
    "import republic_paragraph_parser as paragraph_parser\n",
    "import republic_file_parser as file_parser\n",
    "\n",
    "import copy\n",
    "\n",
    "year = 1725\n",
    "inventory_num = 3767\n",
    "base_config = {\n",
    "    \"inventory_num\": inventory_num,\n",
    "    \"base_dir\": \"../../../Data/Projects/REPUBLIC/hocr/\",\n",
    "    \"page_index\": \"republic_hocr_pages\",\n",
    "    \"page_doc_type\": \"page\",\n",
    "    \"scan_index\": \"republic_hocr_scans\",\n",
    "    \"scan_doc_type\": \"scan\",\n",
    "    \"tiny_word_width\": 15, # pixel width\n",
    "    \"avg_char_width\": 20,\n",
    "    \"remove_tiny_words\": True,\n",
    "    \"remove_line_numbers\": False,\n",
    "    \"normal_scan_width\": 4840\n",
    "}\n",
    "\n",
    "\n",
    "def set_config_inventory_num(base_config, inventory_num):\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"inventory_num\"] = inventory_num\n",
    "    config[\"data_dir\"] = config[\"base_dir\"] + \"{}/\".format(inventory_num)\n",
    "    return config\n",
    "\n",
    "inventory_config = set_config_inventory_num(base_config, inventory_num)\n",
    "print(inventory_config)\n",
    "scan_files = file_parser.get_files(inventory_config[\"data_dir\"])\n",
    "print(\"Number of scan files:\", len(scan_files))\n",
    "\n",
    "scan_file = scan_files[10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Searching of Keywords in the Resolutions\n",
    "\n",
    "Knowing which keywords should appear in the text, possibly with some spelling variation and OCR errors, we can use a fuzzy search algorithm to find candidate matches. \n",
    "\n",
    "Keywords that are similar to each other are registered as distractor terms, so matches are assigned as candidates to the nearest of sets of similar keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parse_republic_hocr_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff7d51402aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mparse_republic_hocr_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge_text_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_hocr_scan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlemma_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'parse_republic_hocr_files'"
     ]
    }
   ],
   "source": [
    "from parse_republic_hocr_files import merge_text_lines, read_hocr_scan\n",
    "\n",
    "lemma_matches = defaultdict(list)\n",
    "\n",
    "def add_context(match, page_text):\n",
    "    context = fuzzy_searcher.get_term_context(page_text, match, context_size=40)\n",
    "    match[\"match_term_in_context\"] = context[\"match_term_in_context\"]\n",
    "    match[\"context_start_offset\"] = context[\"start_offset\"]\n",
    "    match[\"context_end_offset\"] = context[\"end_offset\"]\n",
    "\n",
    "for scan_file in scan_files:\n",
    "    resolution_page_num = scan_file[\"scan_page_num\"] - 90\n",
    "    if scan_file[\"scan_page_num\"] <= 90:\n",
    "        continue\n",
    "    print(scan_file[\"scan_page_num\"], resolution_page_num)\n",
    "    hocr_page = read_hocr_scan(scan_file)\n",
    "    page_text = merge_text_lines(hocr_page)\n",
    "    matches = fuzzy_searcher.find_candidates(page_text)\n",
    "    for match in matches:\n",
    "        lemma_matches[match[\"match_keyword\"]] += [match]\n",
    "        add_context(match, page_text)\n",
    "        match[\"page_num\"] = scan_file[\"scan_page_num\"]\n",
    "        print(match[\"match_keyword\"], \"\\t\", match)\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in sorted(lemma_matches):\n",
    "    print(\"\\n\", lemma, \"\\tAantal kandidaten:\", len(lemma_matches[lemma]), \"\\n\")\n",
    "    for match in lemma_matches[lemma]:\n",
    "        print(\"\\tKandidaat:\", match[\"match_string\"])\n",
    "        print(\"\\tPagina:\", match[\"page_num\"])\n",
    "        print(\"\\tContext:\", match[\"match_term_in_context\"][5:-5])\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
