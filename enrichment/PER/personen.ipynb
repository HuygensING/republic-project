{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "# import republic.model.republic_document_model as rdm\n",
    "import pandas as pd\n",
    "from substitutions import substitutions\n",
    "from deletions import deletions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "repo_dir = os.path.split(os.getcwd())[0]\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Aanmaken en opschonen DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze eerste stap creëren we een DataFrame met de annotaties. Dit DataFrame bevat:\n",
    "\n",
    "- Geaggregeerde verwijzingen: Alle verwijzingen zijn samengevoegd in één DataFrame. Met verwijzingen bedoelen we de output van de NER-tagger, dus de entiteiten zoals ze in de tekst zijn herkend.\n",
    "- Annotatiekolommen: De kolommen `anno_name1` tot en met `anno_name5` zijn toegevoegd om annotaties (standaardversies van achternamen) te registreren.\n",
    "- Subframes: We hebben regels opgesteld om subframes te herkennen, zoals `df_delete` en `df_unmatched`, om te kunnen meten hoeveel rijen er al wel of niet thuisgebracht zijn.\n",
    "\n",
    "Met de stap `df = df[df['aantal'] >= 10]` kunnen we kiezen om te werken met het hele DataFrame, inclusief strings die maar één keer voorkomen, of om te beginnen met vaker voorkomende strings.\n",
    "\n",
    "\n",
    "Hier filter ik de strings van \"annotaties_danig\" eruit, dit zijn namelijk de annotaties die alleen maar hoedanigheden bevatten en geen personen. Dit is afkomstig van het notebook dat hoedanigheden verwerkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load annotations minus hoedanigheden\n",
    "annotations = pd.read_csv('annotations-layer_PER.tsv', sep='\\t')\n",
    "annotations_danig = pd.read_csv('../HOE/matched_hoedanigheden.tsv', sep='\\t')\n",
    "merged_df = pd.merge(\n",
    "    annotations,\n",
    "    annotations_danig[['resolution_id', 'paragraph_id', 'offset', 'end', 'matched']],\n",
    "    on=['resolution_id', 'paragraph_id', 'offset', 'end'],)\n",
    "\n",
    "# Filter out rows where annotations_danig['matched'] has an 'x'\n",
    "rows_to_drop = merged_df[merged_df['matched']]\n",
    "annotations = annotations[~annotations.index.isin(rows_to_drop.index)]\n",
    "print(f'{len(rows_to_drop)} coincide exactly with an attribution; dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['tag_new'] = annotations['tag_text'].apply(lambda x: re.sub(r'^[^a-zA-Z0-9]+', '', x))\n",
    "\n",
    "df_temp = annotations.groupby('tag_new').size().reset_index(name='aantal')\n",
    "column_names = ['tag', 'lowertag', 'aantal', 'anno_name1', 'anno_name2', 'anno_name3',\n",
    "                'anno_name4', 'anno_name5', 'delete', 'best_score', 'select', 'uitzoeken/onduidelijk']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "df['tag'] = df_temp['tag_new']\n",
    "df['aantal'] = df_temp['aantal']\n",
    "print(f\"Dataframe bestaat uit {len(df)} rijen\")\n",
    "df = df[df['aantal'] >= 10] #We doen het nu even meet alles wat meer dan 20 keer voorkomt, om sneller overal doorheen te lopen\n",
    "df['lowertag'] = df['tag'].str.lower()\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "\n",
    "\n",
    "for column in anno_columns:\n",
    "    if df[column].dtype != 'object':  \n",
    "        df[column] = df[column].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken een gestandaardiseerde versie van de verwijzingen aan, waarbij spellingsnormalisatie is toegepast. Dit passen we alleen toe op de \"lowertag\", dat wil zeggen, een kopie van de string die in kleine letters is. De oorspronkelijke strings blijven staan. Door alles in kleine letters te zetten, zijn de tags niet hoofdlettergevoelig. Dit omdat de namen niet altijd met hoofdletters geschreven of herkend zijn.\n",
    "\n",
    "De vervangingen in `substitutions.py` zijn grofweg onder te verdelen in:\n",
    "- leestekens: standaardiseren van komma's, spaties, en verwijderen van onnodige leestekens\n",
    "- Veelvoorkomende woorden: standaardiseren van woorden als \"admiraal\", \"ambassadeur\", \"baron\", \"keurvorst\" e.d. om (delen van) strings makkelijker weg te kunnen zetten als hoedanigheden (en dus geen personen)\n",
    "- Spelling: Om spellingsvariatie te verminderen en matchen makkelijker te maken, vervangen we \"ae\" door \"aa\", \"gt\" door \"cht\" en \"uij\" of \"uy\" door \"ui\".\n",
    "- We hebben ook de verschillende verwijzingen naar afkomst, geboorteplaats en woonplaats gestandaardiseerd, zo wordt bijvoorbeeld \"inwoonder van\" en \"woonende te\" allebei \"woonachtig in\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from substitutions import substitutions\n",
    "\n",
    "def apply_substitutions(df):\n",
    "    # Apply substitutions to DataFrame\n",
    "    substitution_count = 0\n",
    "    for pattern, replacement in tqdm(substitutions.items(), desc=\"Applying substitutions to DataFrame\"):\n",
    "        try:\n",
    "            temp = df['lowertag'].str.count(pattern, flags=re.IGNORECASE)\n",
    "            substitution_count += temp.sum()\n",
    "            df['lowertag'] = df['lowertag'].str.replace(pattern, replacement, regex=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred with pattern: {pattern}\")\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Done! {substitution_count} substitutions made in DataFrame\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = apply_substitutions(df)\n",
    "\n",
    "df.loc[df['lowertag'] == '', 'delete'] = 'x'\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']\n",
    "df_matched = df_zonder_delete[df_zonder_delete[anno_columns].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('fulldfsubs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Koppelen persoonsnamen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De belangrijkste stap in dit notebook is de koppeling: het toepassen van de thesaurus van persoonsnamen op de opgeschoonde strings (`lowertag`). De thesaurus bestaat uit reguliere expressies. De thesaurus is bottom-up opgebouwd uit het materiaal zelf. We ontdekten dat persoonsnamen vaak worden ingeleid door hoedanigheids-woorden, zoals \"admiraal\" of \"lieutenant\", of \"heer\" of \"heeren\". Dit iteratieve proces resulteerde in het kws_dict dictionary, dat bestaat uit reguliere expressies en hun bijbehorende tags, bijvoorbeeld: `\"ro+[ijy]+en\": \"rooyen\"`. Sommige expressies zijn automatisch gegenereerd door algemene regels, zoals het vervangen van de letters u en v door [uv], omdat deze vaak door elkaar worden gebruikt of verkeerd worden herkend. De thesaurus is verder aangevuld met namen uit eerdere corpora, zoals het Repertorium Ambtenaren en Ambtsdragers en Schutte.\n",
    "Door iteratief te werk te gaan, hebben we de expressies steeds verder verfijnd en opgeschoond. Met behulp van Levenshtein-afstand konden we snel zien welke expressies naar dezelfde naam verwezen en deze samenvoegen. Door de output te sorteren op frequentie, konden we gemakkelijk ruis in de vorm van ongewenste tags, zoals `heer van`, identificeren en verwijderen.\n",
    "\n",
    "De koppeling houdt in dat we standaardversies van namen toekennen aan verwijzingen uit de tagger, waarbij de opgeschoonde versie als tussenstap dient. Bijvoorbeeld, `Advocaet Fiscael Bilderbeecq` wordt opgeschoond tot `advocaat fiscaal bilderbeecq`. Dit wordt gematcht met het patroon `bilderb\\w*` en krijgt de annotatie `bilderbeek`.\n",
    "\n",
    "We pasten deze reguliere expressies steeds toe op df_unmatched['lowertag']. Bij een match wordt de tag toegevoegd aan de eerstvolgende beschikbare anno_name kolom, beginnend met `anno_name1`, waardoor de rij wordt toegevoegd aan `df_matched`. Het doel is om bij iedere iteratie de thesaurus uit te breiden en het aantal niet-gematchte verwijzingen te verminderen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze stap passen we annotaties toe op de opgeschoonde gegevens met behulp van reguliere expressies uit `kws_hoe`. Het doel is om relevante tags te identificeren en aan de data toe te voegen.\n",
    "\n",
    "We proberen elk regex-patroon uit de patterns-lijst op de lowertag toe te passen. Als een patroon een match vindt, wordt de string opgedeeld in drie delen: vóór de match [0], de match zelf [1], en na de match [2]. Deze delen worden samen met de overeenkomstige tag en het originele patroon toegevoegd aan een tijdelijk resultaat DataFrame [`new_df`]. We bewaren het originele regex patroon in de kolom `provenance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_annotations(df, kw_dict):\n",
    "    patterns = []\n",
    "\n",
    "    # Compile regex patterns from the dictionary\n",
    "    for pat, repl in kw_dict.items():\n",
    "        try:\n",
    "            pattern_with_group = f\"(.*) *(\\\\b{pat}\\\\b) *(.*)\"\n",
    "            ptrn = re.compile(pattern_with_group, flags=re.I)\n",
    "            patterns.append((ptrn, pat, repl))  # Store the pattern, original 'pat', and replacement\n",
    "        except re.error as e:\n",
    "            print(f\"Error met het patroon '{pattern_with_group}': {e}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame with a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        lowertag = row['lowertag']\n",
    "        aantal = row['aantal']  # Preserve the 'aantal' value\n",
    "        matched = False\n",
    "\n",
    "        for pattern, pat, replacement in patterns:\n",
    "            if matched:\n",
    "                break\n",
    "\n",
    "            match = pattern.match(lowertag)\n",
    "            if match:\n",
    "                matched = True\n",
    "                matches = pd.DataFrame({\n",
    "                    0: [match.group(1)],\n",
    "                    1: [match.group(2)],\n",
    "                    2: [match.group(3)],\n",
    "                    'provenance': [pat],  # Use the original 'pat' for provenance\n",
    "                    'anno_name1': [replacement],  # Use the 'replacement' from kw_dict\n",
    "                    'aantal': [aantal]  # Add the 'aantal' column to the result\n",
    "                }, index=[idx])\n",
    "                results.append(matches)\n",
    "\n",
    "    if results:\n",
    "        offpat = pd.concat(results)\n",
    "        offpat = offpat.sort_values('provenance')\n",
    "    else:\n",
    "        offpat = pd.DataFrame(columns=[0, 1, 2, 'provenance', 'anno_name1', 'aantal'])\n",
    "\n",
    "    return offpat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shortdict3 import kws_dict\n",
    "offpat = apply_annotations(df_unmatched, kws_dict)\n",
    "new_df = offpat\n",
    "print(f\"gematcht: {len(new_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.to_pickle('offpataug20.pkl')\n",
    "#new_df.to_csv('offpat20aug.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de evaluatie te vereenvoudigen, bekijken we elke unieke herkende entiteit-string slechts één keer; dit frame noemen we `preview`. Op deze manier kunnen we snel controleren welke gegevens door de reguliere expressies worden opgevangen, zonder dat het dataframe te lang en onoverzichtelijk wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = new_df.groupby(1).first().reset_index() #bekijk elke unieke string maar 1x\n",
    "preview = preview[new_df.columns]\n",
    "preview.to_csv('previewframe.tsv',sep='\\t')\n",
    "preview.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3a: resultatenframe \"0\" doorzoeken, dus de onderdelen van de entiteiten vóór de herkende string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_pat(df, field, kw_dict, target_name):\n",
    "    results = []\n",
    "\n",
    "    # Compile all patterns beforehand\n",
    "    compiled_patterns = {}\n",
    "    for pattern, replacement in kw_dict.items():\n",
    "        pattern_with_group = f\"(.*) *(\\\\b{pattern}\\\\b) *(.*)\"\n",
    "        compiled_patterns[pattern] = {\n",
    "            'compiled_pattern': re.compile(pattern_with_group, flags=re.I),\n",
    "            'replacement': replacement\n",
    "        }\n",
    "    \n",
    "    # Apply annotations using the compiled patterns\n",
    "    for pattern, data in compiled_patterns.items():\n",
    "        ptrn = data['compiled_pattern']\n",
    "        replacement = data['replacement']\n",
    "        \n",
    "        # Perform regex matching\n",
    "        matches = df[field].str.extractall(ptrn)\n",
    "        \n",
    "        if not matches.empty:\n",
    "            matches['provenance'] = pattern\n",
    "            matches[target_name] = replacement\n",
    "            results.append(matches)\n",
    "\n",
    "    # Return the concatenated DataFrame or an empty DataFrame if no results\n",
    "    if results:\n",
    "        offpat = pd.concat(results)\n",
    "        offpat = offpat.sort_values('provenance')\n",
    "        \n",
    "        # Rename columns based on target_name\n",
    "        if target_name == 'anno_name0':\n",
    "            offpat = offpat.rename(columns={0: 'prov_0'})\n",
    "        elif target_name == 'anno_name2':\n",
    "            offpat = offpat.rename(columns={2: 'prov_2'})\n",
    "        \n",
    "        offpat['old_ind'] = offpat.index.get_level_values(0)  # Keep old indices\n",
    "\n",
    "        return offpat\n",
    "\n",
    "    # Return an empty DataFrame if no matches\n",
    "    return pd.DataFrame(columns=[0, 1, 2, 'provenance', target_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the index and rename columns\n",
    "new_df.index = [idx[0] if isinstance(idx, tuple) else idx for idx in new_df.index]\n",
    "new_df = new_df.rename(columns={'provenance': 'prov_1'})\n",
    "\n",
    "preresults = {}\n",
    "\n",
    "# Use tqdm for a single progress bar\n",
    "for pattern, replacement in tqdm(kws_dict.items(), desc=\"Processing patterns\", unit=\"pattern\"):\n",
    "    target_col = 'anno_0'  # Since there's no distinction, we always target 'anno_0'\n",
    "    \n",
    "    # Call mark_pat with the relevant keyword and pattern\n",
    "    offpat = mark_pat(new_df, field=0, kw_dict={pattern: replacement}, target_name=target_col)\n",
    "\n",
    "    # Only append results if the DataFrame is not empty\n",
    "    if not offpat.empty:\n",
    "        preresults[pattern] = offpat\n",
    "\n",
    "# Concatenate results if there are any\n",
    "if preresults:\n",
    "    preresult_tot = pd.concat(preresults)\n",
    "    print(\"Totaal gevonden:\", len(preresult_tot))\n",
    "else:\n",
    "    preresult_tot = pd.DataFrame(columns=['index', '0'])\n",
    "    print(\"Niets gevonden.\")\n",
    "\n",
    "def ensure_dataframe(df, new_col_name):\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[new_col_name])\n",
    "    else:\n",
    "        df.columns = [new_col_name] + df.columns.tolist()[1:]\n",
    "    return df\n",
    "\n",
    "preresult_tot = ensure_dataframe(preresult_tot, '0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu voegen we de resultaten van deze zoektocht samen met het resultatenframe (new_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize new columns in new_df\n",
    "new_df['anno_0'] = np.nan\n",
    "new_df['prov_0'] = np.nan\n",
    "\n",
    "# Step 2: Function to merge and concatenate annotations, ensuring no duplicates\n",
    "def merge_annotations(src_df, target_df, src_col, tgt_col):\n",
    "    # Create a dictionary to hold sets of aggregated values for uniqueness\n",
    "    temp_dict = {}\n",
    "    for idx, row in src_df.iterrows():\n",
    "        old_ind = row['old_ind']\n",
    "        value = row[src_col]\n",
    "        if old_ind in temp_dict:\n",
    "            temp_dict[old_ind].add(value)\n",
    "        else:\n",
    "            temp_dict[old_ind] = {value}\n",
    "\n",
    "    # Update target_df based on the aggregated dictionary\n",
    "    for index, values in temp_dict.items():\n",
    "        new_value = ', '.join(sorted(values))\n",
    "        if index in target_df.index:\n",
    "            existing_value = target_df.at[index, tgt_col]\n",
    "            if pd.isna(existing_value):\n",
    "                target_df.at[index, tgt_col] = new_value\n",
    "            else:\n",
    "                # Combine existing and new values, avoid duplicates\n",
    "                combined_values = set(existing_value.split(', ')) | values\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(combined_values))\n",
    "\n",
    "# Step 3: Apply the function to each DataFrame\n",
    "merge_annotations(preresult_tot, new_df, 'anno_0', 'anno_0')\n",
    "merge_annotations(preresult_tot, new_df, 'provenance', 'prov_0')\n",
    "\n",
    "# Printing or returning new_df to see the updates\n",
    "# new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 3b: `2` doorzoeken, dus de onderdelen van de entiteiten ná de herkende string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postresults = []\n",
    "\n",
    "# Use tqdm for a single progress bar and process through field=2\n",
    "for pattern, replacement in tqdm(kws_dict.items(), desc=\"Processing postresults\", unit=\"pattern\"):\n",
    "    target_col = 'anno_2'  # Assuming you want to target a different annotation column for postresults\n",
    "    \n",
    "    # Call mark_pat with the relevant keyword and pattern\n",
    "    offpat = mark_pat(new_df, field=2, kw_dict={pattern: replacement}, target_name=target_col)\n",
    "\n",
    "    # Only append results if the DataFrame is not empty\n",
    "    if not offpat.empty:\n",
    "        postresults.append(offpat)\n",
    "\n",
    "# Concatenate results if there are any\n",
    "if postresults:\n",
    "    postresult_tot = pd.concat(postresults, ignore_index=True)\n",
    "    print(\"Totaal gevonden voor postresults:\", len(postresult_tot))\n",
    "else:\n",
    "    postresult_tot = pd.DataFrame(columns=['index', '2'])\n",
    "    print(\"Niets gevonden voor postresults.\")\n",
    "\n",
    "def ensure_dataframe(df, new_col_name):\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[new_col_name])\n",
    "    else:\n",
    "        df.columns = [new_col_name] + df.columns.tolist()[1:]\n",
    "    return df\n",
    "\n",
    "postresult_tot = ensure_dataframe(postresult_tot, '2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Nieuwe kolommen aanmaken\n",
    "new_df['anno_2'] = np.nan  \n",
    "new_df['prov_2'] = np.nan  \n",
    "\n",
    "# Enhanced function to merge and concatenate annotations, ensuring no duplicates\n",
    "def merge_annotations(src_df, target_df, src_col, tgt_col):\n",
    "    # Create a dictionary to hold sets of aggregated values for uniqueness\n",
    "    temp_dict = {}\n",
    "    for idx, row in src_df.iterrows():\n",
    "        old_ind = row['old_ind']\n",
    "        value = row[src_col]\n",
    "        if old_ind in temp_dict:\n",
    "            temp_dict[old_ind].add(value)\n",
    "        else:\n",
    "            temp_dict[old_ind] = {value}\n",
    "\n",
    "    # Update target_df based on the aggregated dictionary\n",
    "    for index, values in temp_dict.items():\n",
    "        if index in target_df.index:\n",
    "            existing_value = target_df.at[index, tgt_col]\n",
    "            if pd.isna(existing_value):\n",
    "                # Directly assign if no existing value\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(values))\n",
    "            else:\n",
    "                # Split existing values, combine with new, ensure uniqueness, then join\n",
    "                existing_set = set(existing_value.split(', '))\n",
    "                combined_values = existing_set | values\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(combined_values))\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "merge_annotations(postresult_tot, new_df, 'anno_2', 'anno_2')\n",
    "merge_annotations(postresult_tot, new_df, 'provenance', 'prov_2')\n",
    "\n",
    "# Tussentijds opslaan\n",
    "new_df.to_pickle('new_df2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het resultatenframe `new_df` is eenvoudig van opzet: het bevat de genormaliseerde strings (`lowertag`), opgesplitst in drie onderdelen (0, 1, 2), en de toegekende annotaties (`anno_name1-5`) en reguliere expressies (`provenance`). In de volgende stap combineren we het geaggregeerde frame `df` met de resultaten uit `new_df`, zodat alle informatie (inclusief de oorspronkelijke tag, aantallen, en nog niet herkende strings) op één centrale plek staat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['provenance'] = np.nan\n",
    "df['prov_0'] = np.nan\n",
    "df['prov_1'] = np.nan\n",
    "df['prov_2'] = np.nan\n",
    "df['prov_2'] = np.nan\n",
    "df['anno_name1'] = new_df['anno_name1'] #where the magic happens\n",
    "df['anno_name3'] = new_df['anno_0'] #where the magic happens\n",
    "df['anno_name2'] = new_df['anno_2'] #where the magic happens\n",
    "\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "df_matched.to_csv('test.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionele tussenstap: bereken welk percentage van de verwijzingen tags heeft gekregen. Dit percentage is relatief aan het vooraf gespecificeerde minimumaantal van voorkomens, in ons geval 10 (zie bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = df['aantal'].sum()\n",
    "total_matched_entities = df_matched['aantal'].sum() + df_delete['aantal'].sum()\n",
    "total_unmatched_entities = df_unmatched['aantal'].sum()\n",
    "\n",
    "# percentage berekenen\n",
    "if total_entities > 0:\n",
    "    percentage_matched = (total_matched_entities / total_entities) * 100\n",
    "else:\n",
    "    percentage_matched = 0\n",
    "\n",
    "print(f\"Hoeveelheid matches: {percentage_matched:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben nu een overzicht van alle door de NER-herkende verwijzingen naar persoonsnamen, en bijbehorende tags die hierbij horen. \n",
    "\n",
    "Ons doel is om in de Goetgevonden-applicatie de tags direct in de tekst weer te geven. Hiervoor moeten we ervoor zorgen dat elke verwijzing een exacte locatie in de tekst heeft, zodat gebruikers op de tekst kunnen klikken en de bijbehorende tag zien. De oorspronkelijke output van de NER-tagger, waarmee we dit notebook hebben geopend, bevat deze informatie.\n",
    "\n",
    "Nu willen we de geaggregeerde tag-informatie terugzetten in dit bestand. Het eindresultaat van dit notebook is om verwijzingen naar persoonsnamen te verrijken met standaardnamen. Deze persoonsnamen moeten genest worden weergegeven (dat wil zeggen, met meerdere, soms overlappende persoonsnamen in de tekst waar relevant).\n",
    "\n",
    "De volgende stappen zijn gericht op het koppelen van de verzamelde informatie aan de oorspronkelijke tekstverwijzingen, inclusief hun exacte locatie in de tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that both columns are strings and strip any leading/trailing whitespace\n",
    "annotations['tag_text'] = annotations['tag_text'].astype(str).str.strip()\n",
    "df_matched['tag'] = df_matched['tag'].astype(str).str.strip()\n",
    "\n",
    "# Perform the merge, keeping all columns from both DataFrames\n",
    "annotations = annotations.merge(df_matched, left_on='tag_text', right_on='tag', how='left', suffixes=('', '_matched'))\n",
    "\n",
    "# Check if the merge has resulted in NaNs and if there are indeed matching values\n",
    "if annotations.isna().all().all():\n",
    "    print(\"Some columns have only NaN values after merging.\")\n",
    "    # Optional: Debug by checking a few mismatches\n",
    "    print(\"Examples of mismatched rows:\")\n",
    "    print(annotations[annotations['tag'].isna()].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations = pd.read_csv('annotations-layer_PER.tsv', sep='\\t')\n",
    "\n",
    "# Apply regex to clean 'tag_text' and create 'tag_new' column\n",
    "annotations['tag_new'] = annotations['tag_text'].str.replace(r'^[^a-zA-Z0-9]+', '', regex=True)\n",
    "\n",
    "# Define the columns that need to be copied from df_matched to annotations\n",
    "columns_to_add = [\n",
    "    'lowertag', 'anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5',\n",
    "    'delete',\n",
    "]\n",
    "\n",
    "# Ensure all required columns are present in annotations\n",
    "for col in columns_to_add:\n",
    "    if col not in annotations.columns:\n",
    "        annotations[col] = np.nan\n",
    "\n",
    "# Normalize 'tag_text' in annotations and 'tag' in df_matched\n",
    "annotations['tag_text'] = annotations['tag_text'].str.strip().str.lower()\n",
    "df_matched['tag'] = df_matched['tag'].str.strip().str.lower()\n",
    "\n",
    "# Remove duplicates in df_matched by keeping the first occurrence of each tag\n",
    "df_matched_unique = df_matched.drop_duplicates(subset='tag', keep='first')\n",
    "\n",
    "# Merge the two DataFrames to add columns from df_matched to annotations\n",
    "# Use a left merge to retain all rows from annotations\n",
    "annotations = annotations.merge(df_matched_unique, left_on='tag_text', right_on='tag', how='left', suffixes=('', '_matched'))\n",
    "\n",
    "# Rename and reorder columns as needed\n",
    "for col in columns_to_add:\n",
    "    if col in annotations.columns:\n",
    "        annotations[col] = annotations[f'{col}_matched']\n",
    "\n",
    "# Drop intermediate columns\n",
    "annotations = annotations.drop(columns=[f'{col}_matched' for col in columns_to_add if f'{col}_matched' in annotations.columns])\n",
    "\n",
    "# Sort the annotations DataFrame by 'anno_name1'\n",
    "annotations = annotations.sort_values('anno_name2')\n",
    "\n",
    "# Display the first 100 rows of the updated annotations DataFrame\n",
    "annotations.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(annotations)\n",
    "\n",
    "# Calculate the number of rows where 'anno_name1' is not empty or NaN\n",
    "non_empty_anno_name1 = annotations['anno_name1'].notna() & (annotations['anno_name1'].str.strip() != '')\n",
    "\n",
    "# Calculate the percentage of rows with non-empty 'anno_name1'\n",
    "percentage_non_empty = non_empty_anno_name1.sum() / total_rows * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of annotations with non-empty 'anno_name1': {percentage_non_empty:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het eindresultaat van dit notebook willen we exporteren als JSON, waarin de tekstverwijzingen samen worden gevoegd met de standaardversies van persoonsnamen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Columns for person names\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "\n",
    "# Filter annotations that have at least one non-null value in the anno_columns\n",
    "annotations_matched = annotations[~annotations[anno_columns].isna().all(axis=1)]\n",
    "\n",
    "# Counter to limit printing\n",
    "print_counter = 0\n",
    "max_prints = 10\n",
    "\n",
    "def create_json_objects(row):\n",
    "    global print_counter\n",
    "    json_objects = []\n",
    "    \n",
    "    for col in anno_columns:\n",
    "        if pd.notnull(row[col]):\n",
    "            # Split the column values by commas\n",
    "            entities = [entity.strip() for entity in str(row[col]).split(',')]\n",
    "\n",
    "            # Print only if there are multiple entities and we haven't exceeded the print limit\n",
    "            if len(entities) > 1 and print_counter < max_prints:\n",
    "                print_counter += 1\n",
    "                print(f\"Found multiple persoonsnamen: {', '.join(entities)}\")\n",
    "\n",
    "            # Iterate through each entity\n",
    "            for entity_name in entities:\n",
    "                provenance = None\n",
    "\n",
    "                # Handle provenance based on the entity column\n",
    "                if col == 'anno_name1':\n",
    "                    provenance = row['prov_1']\n",
    "                elif col == 'anno_name2':\n",
    "                    provenance = row['prov_2']\n",
    "                elif col == 'anno_name3':\n",
    "                    provenance = row['prov_0']\n",
    "\n",
    "                json_object = {\n",
    "                    \"entity\": {\n",
    "                        \"name\": entity_name,\n",
    "                        \"category\": \"PER\",\n",
    "                        \"labels\": []  # No categories provided, keeping an empty list\n",
    "                    },\n",
    "                    \"reference\": {\n",
    "                        \"layer\": row['layer'],\n",
    "                        \"inv\": row['inv'],\n",
    "                        \"tag_text\": row['tag_text'],\n",
    "                        \"resolution_id\": row['resolution_id'],\n",
    "                        \"paragraph_id\": row['paragraph_id'],\n",
    "                        \"offset\": row['offset'],\n",
    "                        \"end\": row['end'],\n",
    "                        \"tag_length\": row['tag_length']\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if provenance and provenance == provenance:\n",
    "                    json_object['reference']['provenance'] = [ provenance ]\n",
    "\n",
    "                json_objects.append(json_object)\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "# Process each row to create JSON objects\n",
    "json_data_list_of_lists = []\n",
    "for index, row in tqdm(annotations_matched.iterrows(), total=len(annotations_matched), desc='Creating JSON objects'):\n",
    "    json_data_list_of_lists.extend(create_json_objects(row))\n",
    "\n",
    "# Write the JSON objects to a file\n",
    "with open('annotations_matched_simple.json', 'w') as json_file:\n",
    "    json.dump(json_data_list_of_lists, json_file, indent=4)\n",
    "\n",
    "print('annotations_matched_simple.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_data_list_of_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
