{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit notebook heeft als doel om NER-herkende verwijzingen naar hoedanigheden te verrijken met standaardnamen. Met hoedanigheden bedoelen we titels, rollen, en functies, zoals \"keizer\" en \"ambassadeur\", maar ook \"weduwe\" en \"moeder\". Dit doen we door de volgende stappen toe te passen:\n",
    "\n",
    "- DataFrame Aanmaken: We creëren een DataFrame met geaggregeerde verwijzingen en annotatiekolommen.\n",
    "- Thesaurus Toepassen: We gebruiken een thesaurus met reguliere expressies om hoedanigheidstags toe te voegen aan de opgeschoonde strings.\n",
    "- Hoedanigheden Herkennen: We zoeken ook naar andere hoedanigheden in delen van de string vóór en na de gevonden hoedanigheid.\n",
    "- Gegevens Samenvoegen: We voegen de geannoteerde gegevens samen met het annotatiebestand en verwijderen ongewenste kolommen.\n",
    "- Resultaten Controleren: We berekenen het percentage van de regels met niet-lege annotaties.\n",
    "\n",
    "Met deze stappen zorgen we ervoor dat alle annotaties en relevante informatie correct zijn gekoppeld en weergegeven in het eindresultaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "# import republic.model.republic_document_model as rdm\n",
    "import pandas as pd\n",
    "from substitutions import substitutions\n",
    "from deletions import deletions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "repo_dir = os.path.split(os.getcwd())[0]\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 1: Aanmaken en opschonen df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze eerste stap creëren we een DataFrame met de annotaties. Dit DataFrame bevat:\n",
    "\n",
    "- Geaggregeerde verwijzingen: Alle verwijzingen zijn samengevoegd in één DataFrame. Met verwijzingen bedoelen we de output van de NER-tagger, dus de entiteiten zoals ze in de tekst zijn herkend.\n",
    "- Annotatiekolommen: De kolommen anno_name1 tot en met anno_name5 zijn toegevoegd om annotaties (hoedanigheden-tags) te registreren.\n",
    "- Subframes: We hebben regels opgesteld om subframes te herkennen, zoals \"df_delete\" en \"df_unmatched\", om te kunnen meten hoeveel rijen er al wel of niet thuisgebracht zijn.\n",
    "\n",
    "Met de stap df = df[df['aantal'] >= 10] kunnen we kiezen om te werken met het hele DataFrame, inclusief strings die maar één keer voorkomen, of om te beginnen met vaker voorkomende strings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('./annotations-layer-HOE.tsv', sep='\\t')\n",
    "annotations['tag_new'] = annotations['tag_text'].apply(lambda x: re.sub(r'^[^a-zA-Z0-9]+', '', x))\n",
    "\n",
    "df_temp = annotations.groupby('tag_new').size().reset_index(name='aantal')\n",
    "column_names = ['tag', 'lowertag', 'aantal', 'anno_name1', 'anno_name2', 'anno_name3',\n",
    "                'anno_name4', 'anno_name5', 'delete', 'best_score', 'select', 'uitzoeken/onduidelijk']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "df['tag'] = df_temp['tag_new']\n",
    "df['aantal'] = df_temp['aantal']\n",
    "df = df[df['aantal'] >= 5] #We doen het nu even meet alles wat meer dan 10 keer voorkomt, om sneller overal doorheen te lopen\n",
    "print(f\"Dataframe bestaat uit {len(df)} rijen\")\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "\n",
    "df['lowertag'] = df['tag'].str.lower()\n",
    "for column in anno_columns:\n",
    "    if df[column].dtype != 'object':  \n",
    "        df[column] = df[column].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken een gestandaardiseerde versie van de hoedanigheden aan, waarbij spellingsnormalisatie is toegepast. Dit passen we alleen toe op de \"lowertag\", dat wil zeggen, een kopie van de string die in kleine letters is. De oorspronkelijke strings blijven staan. Door alles in kleine letters te zetten, zijn de tags niet hoofdlettergevoelig. Dit omdat hoedanigheden in het corpus geen standaardregels gebruiken voor capitalisering.\n",
    "\n",
    "De vervangingen in \"substitutions.py\" zijn grofweg onder te verdelen in:\n",
    "- leestekens: standaardiseren van komma's, spaties, en verwijderen van onnodige leestekens\n",
    "- Veelvoorkomende hoedanigheden: standaardiseren van woorden als \"admiraal\", \"ambassadeur\", \"baron\", \"keurvorst\" e.d. om (delen van) strings makkelijker aan hoedanigheden te matchen\n",
    "- Spelling: Om spellingsvariatie te verminderen en matchen makkelijker te maken, vervangen we \"ae\" door \"aa\", \"gt\" door \"cht\" en \"uij\" of \"uy\" door \"ui\".\n",
    "- We hebben ook de verschillende verwijzingen naar afkomst, geboorteplaats en woonplaats gestandaardiseerd, zo wordt bijvoorbeeld \"inwoonder van\" en \"woonende te\" allebei \"woonachtig in\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from substitutions import substitutions\n",
    "\n",
    "def apply_substitutions(df):\n",
    "    substitution_count = 0 \n",
    "    for pattern, replacement in tqdm(substitutions.items(), desc=\"Vervangingen maken\"):\n",
    "        try:\n",
    "            temp = df['lowertag'].str.count(pattern, flags=re.IGNORECASE) \n",
    "            substitution_count += temp.sum()\n",
    "            df['lowertag'] = df['lowertag'].str.replace(pattern, replacement, regex=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred with pattern: {pattern}\")\n",
    "            print(e)\n",
    "    \n",
    "    print(f\"Klaar! {substitution_count} vervangingen gemaakt\")\n",
    "    return df \n",
    "\n",
    "df = apply_substitutions(df)\n",
    "df.loc[df['lowertag'] == '', 'delete'] = 'x'\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met de dictionary deletions verwijderen we eigennamen en overbodige informatie die ons niet helpt bij het vinden van hoedanigheden. Deze opschoning gebeurt in drie stappen:\n",
    "\n",
    "- Onnodige leestekens en spaties verwijderen: We verwijderen leestekens zoals @/#=„.:,?!\"¬,;:= en overbodige spaties.\n",
    "- Namen verwijderen: We verwijderen namen op basis van de standaardformule \"de heeren van ... tot ...\" en een lijst van veelvoorkomende namen (namen_list) uit andere bronnen. We richten ons alleen op functies, en hoe korter en eenvoudiger de strings, hoe gemakkelijker het is om hoedanigheden te herkennen.\n",
    "- Filler words verwijderen: We verwijderen datums, telwoorden, en woorden zoals \"wijlen\", \"gewesen\", \"geweldigen\", enzovoort, omdat deze ons niet verder helpen.\n",
    "\n",
    "Deze stappen maken de zoekstrings korter en eenvoudiger, wat het proces van het vinden van hoedanigheden efficiënter maakt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deletions import deletions\n",
    "\n",
    "def apply_deletions(df):\n",
    "    deletion_count = 0 \n",
    "    for pattern, replacement in tqdm(deletions.items(), desc=\"Rommel verwijderen\"):\n",
    "        try:\n",
    "            temp = df['lowertag'].str.count(pattern, flags=re.IGNORECASE) \n",
    "            deletion_count += temp.sum()\n",
    "            df['lowertag'] = df['lowertag'].str.replace(pattern, replacement, regex=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred with pattern: {pattern}\")\n",
    "            print(e)\n",
    "    \n",
    "    print(f\"Klaar! {deletion_count} verwijderingen gemaakt\")\n",
    "    return df \n",
    "\n",
    "df = apply_deletions(df)\n",
    "df.loc[df['lowertag'] == '', 'delete'] = 'x'\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n",
    "#df.to_pickle('df_basestaart.pkl') #evt opslaan van dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 2: Koppelen hoedanigheden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De belangrijkste stap in dit notebook is de koppeling: het toepassen van de thesaurus van hoedanigheden op de opgeschoonde strings (lowertag). Dit houdt in dat we hoedanigheidstags toekennen aan verwijzingen uit de tagger, waarbij de opgeschoonde versie als tussenstap dient. Bijvoorbeeld, \"extraord. envoyé aen het Hof van den Keijzer\" krijgt als 'lowertag' \"extraordinaris envoyé aan het hof van de keizer\", met de annotatie-tags (anno_name) \"extraordinaris envoyé\" en \"keizer\".\n",
    "\n",
    "De thesaurus is bottom-up opgebouwd uit het materiaal zelf. We ontdekten dat hoedanigheden vaak worden ingeleid door woorden zoals \"zijnde\" of \"in leven\", of door een komma. We genereerden een lijst van veelvoorkomende unieke woorden die deze patronen volgen en schreven reguliere expressies om verschillende variaties te dekken. Dit resulteerde in het kws_hoe dictionary, dat bestaat uit reguliere expressies en hun bijbehorende tags, bijvoorbeeld: \\\\bextraor\\\\w* *[ie][mn].{0,3}[ijy]+\\\\w*': 'extraordinaris envoyé'.\n",
    "\n",
    "We pasten deze reguliere expressies steeds toe op df_unmatched['lowertag']. Bij een match wordt de tag toegevoegd aan de eerstvolgende beschikbare anno_name kolom, beginnend met anno_name1, waardoor de rij wordt toegevoegd aan df_matched. Het doel is om bij iedere iteratie de thesaurus uit te breiden en het aantal niet-gematchte verwijzingen te verminderen. Zodra het aantal niet-gematchte strings nihil was, verdeelden we kws_hoe grofweg in categorieën. Dit stelt ons in staat om de herkenning van entiteiten in stappen uit te voeren. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cel hieronder is alleen nodig als we hier zouden beginnen, dus met een opgeschoond dataframe van een vorige sessie werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle('df_base3.pkl')\n",
    "df.loc[df['lowertag'] == '', 'delete'] = 'x'\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "len(df_unmatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze stap passen we annotaties toe op de opgeschoonde gegevens met behulp van reguliere expressies uit kw_dict. Het doel is om relevante tags te identificeren en aan de data toe te voegen.\n",
    "\n",
    "We proberen elk regex-patroon uit de patterns-lijst op de lowertag toe te passen.Als een patroon een match vindt, wordt de string opgedeeld in drie delen: vóór de match [0], de match zelf [1], en na de match [2]. Deze delen worden samen met de overeenkomstige tag en het originele patroon toegevoegd aan een tijdelijk resultaat DataFrame [\"new_df\"]. We bewaren het originele regex patroon in de kolom provenance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_annotations(df, kw_dict):\n",
    "    patterns = []\n",
    "\n",
    "    # Compile regex patterns from the dictionary\n",
    "    for pat, repl in kw_dict.items():\n",
    "        try:\n",
    "            pattern_with_group = f\"(.*) *(\\\\b{pat}\\\\b) *(.*)\"\n",
    "            ptrn = re.compile(pattern_with_group, flags=re.I)\n",
    "            patterns.append((ptrn, pat, repl))  # Store the pattern, original 'pat', and replacement\n",
    "        except re.error as e:\n",
    "            print(f\"Error met het patroon '{pattern_with_group}': {e}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame with a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        lowertag = row['lowertag']\n",
    "        aantal = row['aantal']  # Preserve the 'aantal' value\n",
    "        matched = False\n",
    "\n",
    "        for pattern, pat, replacement in patterns:\n",
    "            if matched:\n",
    "                break\n",
    "\n",
    "            match = pattern.match(lowertag)\n",
    "            if match:\n",
    "                matched = True\n",
    "                matches = pd.DataFrame({\n",
    "                    0: [match.group(1)],\n",
    "                    1: [match.group(2)],\n",
    "                    2: [match.group(3)],\n",
    "                    'provenance': [pat],  # Use the original 'pat' for provenance\n",
    "                    'anno_name1': [replacement],  # Use the 'replacement' from kw_dict\n",
    "                    'aantal': [aantal]  # Add the 'aantal' column to the result\n",
    "                }, index=[idx])\n",
    "                results.append(matches)\n",
    "\n",
    "    if results:\n",
    "        offpat = pd.concat(results)\n",
    "        offpat = offpat.sort_values('provenance')\n",
    "    else:\n",
    "        offpat = pd.DataFrame(columns=[0, 1, 2, 'provenance', 'anno_name1', 'aantal'])\n",
    "\n",
    "    return offpat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu lopen we de verschillende categorieën van de hoedanigheden door."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_edelen\n",
    "offpat = apply_annotations(df_unmatched, kw_edelen)\n",
    "new_df = offpat\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n",
    "if new_df.index.has_duplicates:\n",
    "    print(\"Let op! New_df heeft duplicate labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_kerk\n",
    "offpat = apply_annotations(df_unmatched, kw_kerk)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n",
    "if new_df.index.has_duplicates:\n",
    "    print(\"Let op! New_df heeft duplicate labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_ambt\n",
    "offpat = apply_annotations(df_unmatched, kw_ambt)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n",
    "if new_df.index.has_duplicates:\n",
    "    print(\"Let op! New_df heeft duplicate labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_pol\n",
    "offpat = apply_annotations(df_unmatched, kw_pol)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_id\n",
    "offpat = apply_annotations(df_unmatched, kw_id)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_beroep\n",
    "#beroepen\n",
    "offpat = apply_annotations(df_unmatched, kw_beroep)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_rest, kw_new\n",
    "kw_rest.update(kw_new)\n",
    "offpat = apply_annotations(df_unmatched, kw_rest)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kws_hoe import kw_zlast\n",
    "# dingen die je voor het laatst moet bewaren zodat ze geen verwarring veroorzaken\n",
    "offpat = apply_annotations(df_unmatched, kw_zlast)\n",
    "new_df = pd.concat([new_df, offpat])\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.to_pickle('new_dftot.pkl') #tussentijds opslaan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de strings die geen hoedanigheden zijn effectief uit te sluiten, gebruiken we de patronen in \"dellpat\" van kws_hoe. Deze patronen helpen ons bij het identificeren van irrelevante strings. De patronen in dellpat omvatten:\n",
    "\n",
    "- Lege of onbruikbare strings: Bijvoorbeeld '^ *$', wat lege strings identificeert.\n",
    "- Veelvoorkomende niet-relevante termen: Zoals '^ *somme\\b' en '^ *heere?n? *van *\\w* *$', die verwijzen naar algemene termen of plaatsnamen.\n",
    "- Onvolledige of kortere teksten: Bijvoorbeeld '^ *f? *.{0,2} *f? *$', die strings filteren met minder inhoudelijke waarde.\n",
    "\n",
    "Door deze irrelevante strings uit te sluiten, krijgen we een duidelijker beeld van de resterende gegevens in df_unmatched, zonder de ruis mee te tellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hele strings wegzetten als zijnde geen hoedanigheid obv dict\n",
    "from kws_hoe import dellpat\n",
    "df_todel = df_unmatched[df_unmatched['lowertag'].str.contains('|'.join(dellpat), regex=True)]\n",
    "df_todel['delete']='x'\n",
    "len(df_todel)\n",
    "df.update(df_todel)\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "common_indices = new_df.index.get_level_values(0).intersection(df_unmatched.index)\n",
    "df_unmatched = df_unmatched.drop(common_indices)\n",
    "print(f\"gematcht: {len(new_df)}\")\n",
    "print(f\"nog niet gematcht: {len(df_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stap 3**: Na het herkennen van een hoedanigheid kunnen er vaak nog andere hoedanigheden in dezelfde string staan, zowel voor als na de herkende hoedanigheid. Daarom zoeken we ook in de delen van de string vóór en ná de gevonden hoedanigheid (in het resultatenframe heten deze delen respectievelijk \"0\" en \"2\").\n",
    "\n",
    "Bijvoorbeeld, in de string \"extraord. envoyé aen het Hof van den Keijzer\" hebben we \"extraordinaris envoyé\" al herkend, maar we moeten ook \"keizer\" vinden. Deze stap zorgt ervoor dat we alle hoedanigheden in de string identificeren en annoteren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 3a: resultatenframe \"0\" doorzoeken, dus de onderdelen van de entiteiten vóór de herkende string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import kws_hoe\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "\n",
    "def mark_pat(df, field, kw_dict, target_name):\n",
    "    results = []\n",
    "\n",
    "    # Compile all patterns beforehand\n",
    "    compiled_patterns = {}\n",
    "    for pattern, replacement in kw_dict.items():\n",
    "        pattern_with_group = f\"(.*) *(\\\\b{pattern}\\\\b) *(.*)\"\n",
    "        compiled_patterns[pattern] = {\n",
    "            'compiled_pattern': re.compile(pattern_with_group, flags=re.I),\n",
    "            'replacement': replacement\n",
    "        }\n",
    "    \n",
    "    # Apply annotations using the compiled patterns with a progress bar\n",
    "    for pattern, data in tqdm(compiled_patterns.items(), desc=\"Matches zoeken\", unit=\"pattern\"):\n",
    "        ptrn = data['compiled_pattern']\n",
    "        replacement = data['replacement']\n",
    "        \n",
    "        matches = df[field].str.extractall(ptrn)\n",
    "        if not matches.empty:\n",
    "            matches['provenance'] = pattern\n",
    "            matches[target_name] = replacement\n",
    "            results.append(matches)\n",
    "\n",
    "    if results:\n",
    "        offpat = pd.concat(results)\n",
    "        offpat = offpat.sort_values('provenance')\n",
    "        \n",
    "        # Rename columns based on target_name\n",
    "        if target_name == 'anno_name0':\n",
    "            offpat = offpat.rename(columns={0: 'prov_0'})\n",
    "        elif target_name == 'anno_name2':\n",
    "            offpat = offpat.rename(columns={2: 'prov_2'})\n",
    "        elif target_name == 'anno_loc':\n",
    "            offpat = offpat.rename(columns={0: 'prov_loc'})  # Adjust column index as needed\n",
    "        elif target_name == 'anno_qual':\n",
    "            offpat = offpat.rename(columns={2: 'prov_qual'})  # Adjust column index as needed\n",
    "        \n",
    "        offpat['old_ind'] = offpat.index.get_level_values(0)  # Keep old indices\n",
    "\n",
    "    else:\n",
    "        offpat = pd.DataFrame(columns=[0, 1, 2, 'provenance', target_name])\n",
    "\n",
    "    return offpat\n",
    "\n",
    "# Get all dictionaries from kws_hoe module\n",
    "kw_names = {name: value for name, value in inspect.getmembers(kws_hoe, lambda x: isinstance(x, dict)) if not name.startswith(\"__\") and name not in [\"kw_post\", \"kw_pre\", \"kw_mid\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "new_df.index = [idx[0] if isinstance(idx, tuple) else idx for idx in new_df.index]\n",
    "new_df = new_df.rename(columns={'provenance': 'prov_1'})\n",
    "\n",
    "preresults = {}\n",
    "qual_results = {}\n",
    "loc_results = {}\n",
    "\n",
    "for kw in kw_names:\n",
    "    print(\"Kws zoeken:\", kw)\n",
    "    \n",
    "    if kw == 'kws_qual':\n",
    "        target_col = 'anno_qual'\n",
    "    elif kw == 'kws_loc':\n",
    "        target_col = 'anno_loc'\n",
    "    else:\n",
    "        target_col = 'anno_0'\n",
    "    \n",
    "    offpat = mark_pat(new_df, field=0, kw_dict=kw_names[kw], target_name=target_col)\n",
    "    print(\"Aantal gevonden voor\", kw, \":\", len(offpat))\n",
    "\n",
    "    if len(offpat) > 0:\n",
    "        if kw == 'kws_qual':\n",
    "            qual_results[kw] = offpat\n",
    "        elif kw == 'kws_loc':\n",
    "            loc_results[kw] = offpat\n",
    "        else:\n",
    "            preresults[kw] = offpat\n",
    "\n",
    "if preresults:\n",
    "    preresult_tot = pd.concat(preresults)\n",
    "    print(\"Totaal gevonden (exclusief qual):\", len(preresult_tot))\n",
    "else:\n",
    "    preresult_tot = pd.DataFrame(columns=['index', '0'])\n",
    "    print(\"Niets gevonden voor non-qual keywords.\")\n",
    "\n",
    "if qual_results:\n",
    "    qual_result_tot = pd.concat(qual_results)\n",
    "    print(\"Totaal gevonden voor qual:\", len(qual_result_tot))\n",
    "else:\n",
    "    qual_result_tot = pd.DataFrame(columns=['index', '0'])\n",
    "    print(\"Niets gevonden voor qual.\")\n",
    "\n",
    "if loc_results:\n",
    "    loc_result_tot = pd.concat(loc_results)\n",
    "    print(\"Totaal gevonden voor loc:\", len(loc_result_tot))\n",
    "else:\n",
    "    loc_result_tot = pd.DataFrame(columns=['index', '0'])\n",
    "    print(\"Niets gevonden voor loc.\")\n",
    "\n",
    "def ensure_dataframe(df, new_col_name):\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[new_col_name])\n",
    "    else:\n",
    "        df.columns = [new_col_name] + df.columns.tolist()[1:]\n",
    "    return df\n",
    "\n",
    "preresult_tot = ensure_dataframe(preresult_tot, '0')\n",
    "qual_result_tot = ensure_dataframe(qual_result_tot, '0')\n",
    "loc_result_tot = ensure_dataframe(loc_result_tot, '0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu voegen we de resultaten van deze zoektocht samen met het resultatenframe (new_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize new columns in new_df\n",
    "new_df['anno_0'] = np.nan\n",
    "new_df['anno_loc'] = np.nan\n",
    "new_df['anno_qual'] = np.nan\n",
    "new_df['prov_0'] = np.nan\n",
    "new_df['prov_qual'] = np.nan\n",
    "new_df['prov_loc'] = np.nan\n",
    "\n",
    "\n",
    "# Step 2: Function to merge and concatenate annotations, ensuring no duplicates\n",
    "def merge_annotations(src_df, target_df, src_col, tgt_col):\n",
    "    # Create a dictionary to hold sets of aggregated values for uniqueness\n",
    "    temp_dict = {}\n",
    "    for idx, row in src_df.iterrows():\n",
    "        old_ind = row['old_ind']\n",
    "        value = row[src_col]\n",
    "        if old_ind in temp_dict:\n",
    "            temp_dict[old_ind].add(value)\n",
    "        else:\n",
    "            temp_dict[old_ind] = {value}\n",
    "\n",
    "    # Update target_df based on the aggregated dictionary\n",
    "    for index, values in temp_dict.items():\n",
    "        new_value = ', '.join(sorted(values))\n",
    "        if index in target_df.index:\n",
    "            existing_value = target_df.at[index, tgt_col]\n",
    "            if pd.isna(existing_value):\n",
    "                target_df.at[index, tgt_col] = new_value\n",
    "            else:\n",
    "                # Combine existing and new values, avoid duplicates\n",
    "                combined_values = set(existing_value.split(', ')) | values\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(combined_values))\n",
    "\n",
    "# Step 3: Apply the function to each DataFrame\n",
    "merge_annotations(preresult_tot, new_df, 'anno_0', 'anno_0')\n",
    "merge_annotations(preresult_tot, new_df, 'provenance', 'prov_0')\n",
    "merge_annotations(qual_result_tot, new_df, 'anno_qual', 'anno_qual')\n",
    "merge_annotations(qual_result_tot, new_df, 'provenance', 'prov_qual')\n",
    "merge_annotations(loc_result_tot, new_df, 'anno_loc', 'anno_loc')\n",
    "merge_annotations(loc_result_tot, new_df, 'provenance', 'prov_loc')\n",
    "\n",
    "# Printing or returning new_df to see the updates\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 3b: \"2\" doorzoeken, dus de onderdelen van de entiteiten ná de herkende string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ongeveer 20 seconden\n",
    "postresults = {}\n",
    "qual_results = {}\n",
    "loc_results = {}\n",
    "\n",
    "for kw in kw_names:\n",
    "    print(\"Kws zoeken:\", kw)\n",
    "    \n",
    "    if kw == 'kws_qual':\n",
    "        target_col = 'anno_qual_2'\n",
    "    elif kw == 'kws_loc':\n",
    "        target_col = 'anno_loc_2'\n",
    "    else:\n",
    "        target_col = 'anno_2'\n",
    "    \n",
    "    offpat = mark_pat(new_df, field=2, kw_dict=kw_names[kw], target_name=target_col)\n",
    "    print(\"Aantal gevonden voor\", kw, \":\", len(offpat))\n",
    "\n",
    "    if len(offpat) > 0:\n",
    "        if kw == 'kws_qual':\n",
    "            qual_results[kw] = offpat\n",
    "        elif kw == 'kws_loc':\n",
    "            loc_results[kw] = offpat\n",
    "        else:\n",
    "            postresults[kw] = offpat\n",
    "\n",
    "if postresults:\n",
    "    postresult_tot = pd.concat(postresults)\n",
    "    print(\"Totaal gevonden (exclusief qual):\", len(postresult_tot))\n",
    "else:\n",
    "    postresult_tot = pd.DataFrame(columns=['index', '2'])\n",
    "    print(\"Niets gevonden voor non-qual keywords.\")\n",
    "\n",
    "if qual_results:\n",
    "    qual_result_tot = pd.concat(qual_results)\n",
    "    print(\"Totaal gevonden voor qual:\", len(qual_result_tot))\n",
    "else:\n",
    "    qual_result_tot = pd.DataFrame(columns=['index', '2'])\n",
    "    print(\"Niets gevonden voor qual.\")\n",
    "\n",
    "if loc_results:\n",
    "    loc_result_tot = pd.concat(loc_results)\n",
    "    print(\"Totaal gevonden voor loc:\", len(loc_result_tot))\n",
    "else:\n",
    "    loc_result_tot = pd.DataFrame(columns=['index', '2'])\n",
    "    print(\"Niets gevonden voor loc.\")\n",
    "\n",
    "def ensure_dataframe(df, new_col_name):\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[new_col_name])\n",
    "    else:\n",
    "        df.columns = [new_col_name] + df.columns.tolist()[1:]\n",
    "    return df\n",
    "\n",
    "preresult_tot = ensure_dataframe(preresult_tot, '2')\n",
    "postresult_tot = ensure_dataframe(postresult_tot, '2')\n",
    "qual_result_tot = ensure_dataframe(qual_result_tot, '2')\n",
    "loc_result_tot = ensure_dataframe(loc_result_tot, '2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Nieuwe kolommen aanmaken\n",
    "new_df['anno_2'] = np.nan  \n",
    "new_df['anno_qual_2'] = np.nan  \n",
    "new_df['anno_loc_2'] = np.nan  \n",
    "new_df['prov_qual_2'] = np.nan  \n",
    "new_df['prov_loc_2'] = np.nan \n",
    "new_df['prov_2'] = np.nan  \n",
    "\n",
    "# Enhanced function to merge and concatenate annotations, ensuring no duplicates\n",
    "def merge_annotations(src_df, target_df, src_col, tgt_col):\n",
    "    # Create a dictionary to hold sets of aggregated values for uniqueness\n",
    "    temp_dict = {}\n",
    "    for idx, row in src_df.iterrows():\n",
    "        old_ind = row['old_ind']\n",
    "        value = row[src_col]\n",
    "        if old_ind in temp_dict:\n",
    "            temp_dict[old_ind].add(value)\n",
    "        else:\n",
    "            temp_dict[old_ind] = {value}\n",
    "\n",
    "    # Update target_df based on the aggregated dictionary\n",
    "    for index, values in temp_dict.items():\n",
    "        if index in target_df.index:\n",
    "            existing_value = target_df.at[index, tgt_col]\n",
    "            if pd.isna(existing_value):\n",
    "                # Directly assign if no existing value\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(values))\n",
    "            else:\n",
    "                # Split existing values, combine with new, ensure uniqueness, then join\n",
    "                existing_set = set(existing_value.split(', '))\n",
    "                combined_values = existing_set | values\n",
    "                target_df.at[index, tgt_col] = ', '.join(sorted(combined_values))\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "merge_annotations(postresult_tot, new_df, 'anno_2', 'anno_2')\n",
    "merge_annotations(postresult_tot, new_df, 'provenance', 'prov_2')\n",
    "merge_annotations(qual_result_tot, new_df, 'anno_qual_2', 'anno_qual_2')\n",
    "merge_annotations(qual_result_tot, new_df, 'provenance', 'prov_qual_2')\n",
    "merge_annotations(loc_result_tot, new_df, 'anno_loc_2', 'anno_loc_2')\n",
    "merge_annotations(loc_result_tot, new_df, 'provenance', 'prov_loc_2')\n",
    "\n",
    "\n",
    "# Tussentijds opslaan\n",
    "#new_df.to_pickle('new_dfstaart2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hebben we alle verwijzingen naar hoedanigheden thuisgebracht en geannoteerd. De volgende stap is overkoepelende categorieën toekennen aan de verwijzingen op basis van een handmatig samengestelde lijst. \n",
    "Deze categorieën [LICHT TOE: welke categorieën zijn het, welk doel hebben ze in de uiteindelijke applicatie?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = pd.read_csv('cats.tsv', sep='\\t', header=0) #Handmatige lijst met categorie-toewijzingen inlezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Ruimte maken voor de categorielabels\n",
    "new_df['cat1'] = np.nan\n",
    "new_df['cat0'] = np.nan\n",
    "new_df['cat2'] = np.nan\n",
    "\n",
    "# Step 2: Zoeken\n",
    "def label_column_with_priority(row, anno_column):\n",
    "    if pd.isna(row[anno_column]):\n",
    "        return np.nan\n",
    "\n",
    "    # Check for a match in the 'Rood' column\n",
    "    match = cats_df.loc[cats_df['Rood'] == row[anno_column], 'Cat_1_nieuw']\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]  # Return the first match\n",
    "\n",
    "    # If no match in 'Rood', check for a match in the 'Hoedanigheid' column\n",
    "    match = cats_df.loc[cats_df['Hoedanigheid'] == row[anno_column], 'Cat_1_nieuw']\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]  # Return the first match\n",
    "\n",
    "    # If no match is found, return 'Rest'\n",
    "    return 'Rest'\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "tqdm.pandas(desc=\"Categorieën toekennen\")\n",
    "new_df['cat1'] = new_df.progress_apply(lambda row: label_column_with_priority(row, 'anno_name1'), axis=1)\n",
    "new_df['cat0'] = new_df.progress_apply(lambda row: label_column_with_priority(row, 'anno_0'), axis=1)\n",
    "new_df['cat2'] = new_df.progress_apply(lambda row: label_column_with_priority(row, 'anno_2'), axis=1)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "#new_df.to_pickle('new_dffull.pkl')\n",
    "#new_df.to_csv('updated_data.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het resultatenframe `new_df` is eenvoudig van opzet: het bevat de genormaliseerde strings (`lowertag`), opgesplitst in drie onderdelen (0, 1, 2), en de toegekende annotaties (`anno_name1-5`) en reguliere expressies (`provenance`). In de volgende stap combineren we het geaggregeerde frame `df` met de resultaten uit `new_df`, zodat alle informatie (inclusief de oorspronkelijke tag, aantallen, en nog niet herkende strings) op één centrale plek staat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.read_pickle('new_dffull.pkl') #eventuele stap als je hierboven het resultatenframe hebt opgeslagen en nu weer hier wilt beginnen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['cat0'] = np.nan\n",
    "df['cat1'] = np.nan\n",
    "df['cat2'] = np.nan\n",
    "df['provenance'] = np.nan\n",
    "df['prov_0'] = np.nan\n",
    "df['prov_1'] = np.nan\n",
    "df['prov_2'] = np.nan\n",
    "df['prov_loc'] = np.nan\n",
    "df['anno_qual'] = np.nan\n",
    "df['anno_loc'] = np.nan\n",
    "df['anno_qual_2'] = np.nan\n",
    "df['anno_loc_2'] = np.nan\n",
    "df['prov_qual_2'] = np.nan\n",
    "df['prov_loc_2'] = np.nan\n",
    "df['prov_2'] = np.nan\n",
    "df['anno_name1'] = new_df['anno_name1'] #where the magic happens\n",
    "df['anno_loc'] = new_df['anno_loc'] #where the magic happens\n",
    "df['anno_loc_2'] = new_df['anno_loc_2'] #where the magic happens\n",
    "df['anno_qual'] = new_df['anno_qual'] #where the magic happens\n",
    "df['anno_qual_2'] = new_df['anno_qual_2'] #where the magic happens\n",
    "df['anno_name3'] = new_df['anno_0'] #where the magic happens\n",
    "df['anno_name2'] = new_df['anno_2'] #where the magic happens\n",
    "df['cat0'] = new_df['cat0']\n",
    "df['cat2'] = new_df['cat2'] #where the magic happens\n",
    "df['cat1'] = new_df['cat1'] \n",
    "df['prov_loc'] = new_df['prov_loc']\n",
    "df['prov_0'] = new_df['prov_0']\n",
    "df['prov_1'] = new_df['prov_1']\n",
    "df['prov_2'] = new_df['prov_2']\n",
    "\n",
    "df_delete = df[df['delete'] == 'x']\n",
    "df_zonder_delete = df[~(df['delete'] == 'x')]\n",
    "df_matched = df_zonder_delete[df_zonder_delete[['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4']].notna().any(axis=1)]\n",
    "df_unmatched = df_zonder_delete[df_zonder_delete[anno_columns].isna().all(axis=1) & ~df['delete'].astype(str).eq('x')]\n",
    "df_matched.to_csv('sep.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionele tussenstap: bereken welk percentage van de verwijzingen tags heeft gekregen. Dit percentage is relatief aan het vooraf gespecificeerde minimumaantal van voorkomens, in ons geval 10 (zie bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = df['aantal'].sum()\n",
    "total_matched_entities = df_matched['aantal'].sum() + df_delete['aantal'].sum()\n",
    "total_unmatched_entities = df_unmatched['aantal'].sum()\n",
    "\n",
    "# percentage berekenen\n",
    "if total_entities > 0:\n",
    "    percentage_matched = (total_matched_entities / total_entities) * 100\n",
    "else:\n",
    "    percentage_matched = 0\n",
    "\n",
    "print(f\"Hoeveelheid matches: {percentage_matched:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben nu een overzicht van alle door de NER-herkende verwijzingen naar hoedanigheden, en bijbehorende tags die hierbij horen. \n",
    "\n",
    "Ons doel is om in de Goetgevonden-applicatie de tags direct in de tekst weer te geven. Hiervoor moeten we ervoor zorgen dat elke verwijzing een exacte locatie in de tekst heeft, zodat gebruikers op de tekst kunnen klikken en de bijbehorende tag zien. De oorspronkelijke output van de NER-tagger, waarmee we dit notebook hebben geopend, bevat deze informatie.\n",
    "\n",
    "Nu willen we de geaggregeerde tag-informatie terugzetten in dit bestand. Het eindresultaat van dit notebook is om verwijzingen naar hoedanigheden te verrijken met standaardnamen. Deze hoedanigheden moeten genest worden weergegeven (dat wil zeggen, met meerdere, soms overlappende hoedanigheden in de tekst waar relevant) en gecategoriseerd worden in overkoepelende groepen zoals adel & vorstenhuizen, politiek & bestuur, enzovoort.\n",
    "\n",
    "De volgende stappen zijn gericht op het koppelen van de verzamelde informatie aan de oorspronkelijke tekstverwijzingen, inclusief hun exacte locatie in de tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that both columns are strings and strip any leading/trailing whitespace\n",
    "annotations['tag_text'] = annotations['tag_text'].astype(str).str.strip()\n",
    "df_matched['tag'] = df_matched['tag'].astype(str).str.strip()\n",
    "\n",
    "# Perform the merge, keeping all columns from both DataFrames\n",
    "annotations = annotations.merge(df_matched, left_on='tag_text', right_on='tag', how='left', suffixes=('', '_matched'))\n",
    "\n",
    "# Check if the merge has resulted in NaNs and if there are indeed matching values\n",
    "if annotations.isna().all().all():\n",
    "    print(\"Some columns have only NaN values after merging.\")\n",
    "    # Optional: Debug by checking a few mismatches\n",
    "    print(\"Examples of mismatched rows:\")\n",
    "    print(annotations[annotations['tag'].isna()].head())\n",
    "\n",
    "# Display the first 10 rows of the merged DataFrame\n",
    "annotations.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations = pd.read_csv('./annotations-layer-HOE.tsv', sep='\\t')\n",
    "\n",
    "# Apply regex to clean 'tag_text' and create 'tag_new' column\n",
    "annotations['tag_new'] = annotations['tag_text'].str.replace(r'^[^a-zA-Z0-9]+', '', regex=True)\n",
    "\n",
    "# Define the columns that need to be copied from df_matched to annotations\n",
    "columns_to_add = [\n",
    "    'lowertag', 'anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5',\n",
    "    'delete', 'best_score', 'select', 'uitzoeken/onduidelijk', 'anno_loc', \n",
    "    'anno_qual', 'cat0', 'cat1', 'cat2', 'provenance', 'prov_0', 'prov_1', \n",
    "    'prov_2', 'prov_loc', 'anno_qual_2', 'anno_loc_2', 'prov_qual_2', 'prov_loc_2'\n",
    "]\n",
    "\n",
    "# Ensure all required columns are present in annotations\n",
    "for col in columns_to_add:\n",
    "    if col not in annotations.columns:\n",
    "        annotations[col] = np.nan\n",
    "\n",
    "# Normalize 'tag_text' in annotations and 'tag' in df_matched\n",
    "annotations['tag_text'] = annotations['tag_text'].str.strip().str.lower()\n",
    "df_matched['tag'] = df_matched['tag'].str.strip().str.lower()\n",
    "\n",
    "# Remove duplicates in df_matched by keeping the first occurrence of each tag\n",
    "df_matched_unique = df_matched.drop_duplicates(subset='tag', keep='first')\n",
    "\n",
    "# Merge the two DataFrames to add columns from df_matched to annotations\n",
    "# Use a left merge to retain all rows from annotations\n",
    "annotations = annotations.merge(df_matched_unique, left_on='tag_text', right_on='tag', how='left', suffixes=('', '_matched'))\n",
    "\n",
    "# Rename and reorder columns as needed\n",
    "for col in columns_to_add:\n",
    "    if col in annotations.columns:\n",
    "        annotations[col] = annotations[f'{col}_matched']\n",
    "\n",
    "# Drop intermediate columns\n",
    "annotations = annotations.drop(columns=[f'{col}_matched' for col in columns_to_add if f'{col}_matched' in annotations.columns])\n",
    "\n",
    "# Sort the annotations DataFrame by 'anno_name1'\n",
    "annotations = annotations.sort_values('anno_name1')\n",
    "\n",
    "# Display the first 100 rows of the updated annotations DataFrame\n",
    "annotations.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(annotations)\n",
    "\n",
    "# Calculate the number of rows where 'anno_name1' is not empty or NaN\n",
    "non_empty_anno_name1 = annotations['anno_name1'].notna() & (annotations['anno_name1'].str.strip() != '')\n",
    "\n",
    "# Calculate the percentage of rows with non-empty 'anno_name1'\n",
    "percentage_non_empty = non_empty_anno_name1.sum() / total_rows * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of annotations with non-empty 'anno_name1': {percentage_non_empty:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het eindresultaat van dit notebook willen we exporteren als JSON, waarin de tekstverwijzingen samen worden gevoegd met de standaardversies van hoedanigheden en overkoepelende categorieën.  \n",
    "\n",
    "In dit project hebben we twee versies van JSON-bestanden gemaakt om de annotaties van hoedanigheden te representeren. Deze annotaties kunnen kwalificaties en locaties bevatten die aan een entiteit zijn gekoppeld. De kwalificaties en locaties worden op twee manieren verwerkt:\n",
    "\n",
    "Met kwalificaties en locaties tussen haakjes:\n",
    "\n",
    "In deze versie worden kwalificaties en locaties toegevoegd aan de naam van de entiteit, tussen haakjes. Bijvoorbeeld:\n",
    "\"plenipotentiaris (vice)\"\n",
    "\"generaal (engels)\"\n",
    "Deze aanpak maakt het mogelijk om extra context of specificatie direct bij de naam van de entiteit te voegen, wat helpt bij het verduidelijken van de rol of functie van de entiteit binnen het document.\n",
    "\n",
    "Zonder kwalificaties en locaties tussen haakjes:\n",
    "\n",
    "In deze versie worden kwalificaties en locaties niet tussen haakjes toegevoegd aan de naam van de entiteit. Alleen de basisnaam van de entiteit wordt opgenomen.\n",
    "Dit kan nuttig zijn wanneer je een eenvoudiger formaat nodig hebt zonder extra context, of wanneer de kwalificaties en locaties niet van belang zijn voor je toepassing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming annotations and anno_columns are already defined\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5',\n",
    "                'anno_loc', 'anno_loc_2', 'anno_qual', 'anno_qual_2']\n",
    "cat_columns = ['cat1', 'cat2', 'cat0', 'cat0', 'cat0']  # cat0 mapped to anno_name3\n",
    "\n",
    "annotations_matched = annotations\n",
    "\n",
    "# Update create_json_objects function to include category labels and provenance\n",
    "def create_json_objects(row):\n",
    "    json_objects = []\n",
    "    for col, cat_col in zip(anno_columns, cat_columns):\n",
    "        if pd.notnull(row[col]):\n",
    "            category_label = row[cat_col]\n",
    "            provenance = None\n",
    "\n",
    "            # Modify entity name and provenance for qualifiers and locations\n",
    "            if col == 'anno_qual' and pd.notnull(row['anno_name1']):\n",
    "                entity_name = f\"{row['anno_name1']} ({row['anno_qual']})\"\n",
    "                provenance = f\"{row['prov_1']}, {row['prov_qual']}\"\n",
    "            elif col == 'anno_qual_2':\n",
    "                if pd.notnull(row['anno_name2']):\n",
    "                    entity_name = f\"{row['anno_name2']} ({row['anno_qual_2']})\"\n",
    "                    provenance = f\"{row['prov_2']}, {row['prov_qual_2']}\"\n",
    "                else:\n",
    "                    entity_name = f\"{row['anno_name1']} ({row['anno_qual_2']})\"\n",
    "                    provenance = f\"{row['prov_1']}, {row['prov_qual_2']}\"\n",
    "            elif col == 'anno_loc' and pd.notnull(row['anno_name1']):\n",
    "                entity_name = f\"{row['anno_name1']} ({row['anno_loc']})\"\n",
    "                provenance = f\"{row['prov_1']}, {row['prov_loc']}\"\n",
    "            elif col == 'anno_loc_2':\n",
    "                if pd.notnull(row['anno_name2']):\n",
    "                    entity_name = f\"{row['anno_name2']} ({row['anno_loc_2']})\"\n",
    "                    provenance = f\"{row['prov_2']}, {row['prov_loc_2']}\"\n",
    "                else:\n",
    "                    entity_name = f\"{row['anno_name1']} ({row['anno_loc_2']})\"\n",
    "                    provenance = f\"{row['prov_1']}, {row['prov_loc_2']}\"\n",
    "            else:\n",
    "                entity_name = row[col]\n",
    "                if col == 'anno_name1':\n",
    "                    provenance = row['prov_1']\n",
    "                elif col == 'anno_name2':\n",
    "                    provenance = row['prov_2']\n",
    "                elif col == 'anno_name3':\n",
    "                    provenance = row['prov_0']\n",
    "            \n",
    "            json_object = {\n",
    "                \"entity\": {\n",
    "                    \"name\": entity_name,\n",
    "                    \"category\": \"HOE\",\n",
    "                    \"labels\": [category_label] if pd.notnull(category_label) else []\n",
    "                },\n",
    "                \"reference\": {\n",
    "                    \"layer\": row['layer'],\n",
    "                    \"inv\": row['inv'],\n",
    "                    \"tag_text\": row['tag_text'],\n",
    "                    \"resolution_id\": row['resolution_id'],\n",
    "                    \"paragraph_id\": row['paragraph_id'],\n",
    "                    \"offset\": row['offset'],\n",
    "                    \"end\": row['end'],\n",
    "                    \"tag_length\": row['tag_length']\n",
    "                }\n",
    "            }\n",
    "            if provenance:\n",
    "                json_object['reference']['provenance'] = [ provenance ]\n",
    "            \n",
    "            json_objects.append(json_object)\n",
    "    return json_objects\n",
    "\n",
    "# Generate JSON objects\n",
    "json_data_list_of_lists = []\n",
    "for index, row in tqdm(annotations_matched.iterrows(), total=len(annotations_matched), desc='Creating JSON objects'):\n",
    "    json_data_list_of_lists.extend(create_json_objects(row))\n",
    "\n",
    "# Save JSON to file\n",
    "with open('annotations_matched_qualloc.json', 'w') as json_file:\n",
    "    json.dump(json_data_list_of_lists, json_file, indent=4)\n",
    "\n",
    "print('annotations_matched_qualloc.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json2: zonder loc en qual tussen haakjes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# zonder loc en qual\n",
    "anno_columns = ['anno_name1', 'anno_name2', 'anno_name3', 'anno_name4', 'anno_name5']\n",
    "cat_columns = ['cat1', 'cat2', 'cat0', 'cat0', 'cat0']  # cat0 mapped to anno_name3\n",
    "\n",
    "annotations_matched = annotations[~annotations[anno_columns].isna().all(axis=1)]\n",
    "\n",
    "def create_json_objects(row):\n",
    "    json_objects = []\n",
    "    for col, cat_col in zip(anno_columns, cat_columns):\n",
    "        if pd.notnull(row[col]):\n",
    "            category_label = row[cat_col]\n",
    "            provenance = None\n",
    "\n",
    "            entity_name = row[col]\n",
    "            if col == 'anno_name1':\n",
    "                provenance = row['prov_1']\n",
    "            elif col == 'anno_name2':\n",
    "                provenance = row['prov_2']\n",
    "            elif col == 'anno_name3':\n",
    "                provenance = row['prov_0']\n",
    "\n",
    "            json_object = {\n",
    "                \"entity\": {\n",
    "                    \"name\": entity_name,\n",
    "                    \"category\": \"HOE\",\n",
    "                    \"labels\": [category_label] if pd.notnull(category_label) else []\n",
    "                },\n",
    "                \"reference\": {\n",
    "                    \"layer\": row['layer'],\n",
    "                    \"inv\": row['inv'],\n",
    "                    \"tag_text\": row['tag_text'],\n",
    "                    \"resolution_id\": row['resolution_id'],\n",
    "                    \"paragraph_id\": row['paragraph_id'],\n",
    "                    \"offset\": row['offset'],\n",
    "                    \"end\": row['end'],\n",
    "                    \"tag_length\": row['tag_length']\n",
    "                }\n",
    "            }\n",
    "            if provenance:\n",
    "                json_object['reference']['provenance'] = [ provenance ]\n",
    "            \n",
    "            json_objects.append(json_object)\n",
    "    return json_objects\n",
    "\n",
    "json_data_list_of_lists = []\n",
    "for index, row in tqdm(annotations_matched.iterrows(), total=len(annotations_matched), desc='Creating JSON objects'):\n",
    "    json_data_list_of_lists.extend(create_json_objects(row))\n",
    "\n",
    "with open('annotations_matched_simple.json', 'w') as json_file:\n",
    "    json.dump(json_data_list_of_lists, json_file, indent=4)\n",
    "\n",
    "print('annotations_matched_simple.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we export a list of matched rows for use by the name recognition notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_matched(cols):\n",
    "    return any([x==x for x in list(cols)])\n",
    "annotations['matched'] = annotations[anno_columns].apply(is_matched, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[['layer','resolution_id','paragraph_id','offset','end','matched']\n",
    "].to_csv('matched_hoedanigheden.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
